# Publication Framing Guide

**How to present Embody as translational research in academic contexts**

---

## Purpose

This guide provides concrete templates and framing strategies for presenting Embody in thesis chapters, conference papers, journal articles, and talks. Use these when the audience needs explicit "this demonstrates" explanations that don't belong in the technical documentation itself.

---

## Thesis Introduction Template

**Opening paragraph**:

> This thesis presents translational computing education research that spans multiple phases of the TCER model (Cole, Malaise, & Signer, 2023). The work makes three primary research contributions: (1) **theoretical**—12 JavaScript-specific notional machines (Phase 1.B); (2) **synthesizing**—comprehensive literature reviews and practitioner guidelines (Phases 3.A/B*); and (3) **methodological**—a replicable theory-to-requirements process for infrastructure research (spanning Phases 1-4). The Embody execution tracer emerges from this process as a Phase 4.A* infrastructure prototype that enables future empirical work (Phase 2) and scaled interventions (Phase 5).

**Structure for thesis chapters**:

1. **Chapter 1: Introduction**
   - Present TCER framework briefly
   - Position Embody as multi-phase work
   - Preview three contribution types

2. **Chapter 2: Literature Review** (Phase 3.A contribution)
   - Present as research synthesis, not background
   - Evidence hierarchy methodology
   - 30+ paper analysis

3. **Chapter 3: JavaScript Notional Machines** (Phase 1.B contribution)
   - Domain-specific theory development
   - Systematic extrapolation methodology
   - 12 notional machines with validation status

4. **Chapter 4: Theory-to-Requirements Methodology** (Methodological contribution)
   - 5-step translational process
   - Replicable pattern for infrastructure projects
   - Maps to TCER phases

5. **Chapter 5: Practitioner Guidelines** (Phase 3.B* contribution)
   - Taxonomies and use cases
   - Trading zone function
   - Tool developer guidance

6. **Chapter 6: Embody Infrastructure** (Phase 4.A* contribution)
   - Evidence-based prototype
   - Configuration API design
   - Neutral infrastructure principle

7. **Chapter 7: Conclusion**
   - Multi-phase nature as strength
   - Future work (Phases 2, 4.B, 5)
   - Contribution to CER methodology

---

## Conference Paper Abstract Template

**Full abstract** (150-200 words):

> We present a translational approach to educational infrastructure research, demonstrated through the development of Embody, a configurable JavaScript execution tracer. Our theory-to-requirements methodology integrates domain-specific theory development (TCER Phase 1.B) with infrastructure prototyping (Phase 4.A*), producing three research contributions: 12 JavaScript-specific notional machines, systematic practitioner guidelines, and a replicable methodology for similar projects. We situate this work within the Translational Computing Education Research framework, showing how infrastructure research operates in trading zones between researchers, practitioners, and tool developers. This approach addresses a methodological gap in CER for systematically developing measurement infrastructure. The resulting tracer provides neutral, configurable execution data that enables diverse pedagogical approaches—from constructivist exploration to direct instruction—without encoding a specific educational theory. We demonstrate how tool-theory co-evolution drives infrastructure development, argue that building measurement tools constitutes research contribution, and provide a replicable pattern for other domain-specific infrastructure projects (type systems, collaboration tools, debugging support).

**Short abstract** (75-100 words):

> We present Embody, a configurable JavaScript execution tracer developed through translational computing education research spanning TCER Phases 1.B (domain theories), 3.A/B* (synthesis), and 4.A* (infrastructure). Our contribution is threefold: (1) 12 JavaScript-specific notional machines, (2) systematic practitioner guidelines, and (3) a replicable theory-to-requirements methodology. We demonstrate how infrastructure research operates in trading zones, enabling diverse pedagogical approaches through neutral trace data. This work addresses CER's methodological gap in systematic infrastructure development and provides a pattern others can adapt for type systems, collaboration tools, and debugging support.

---

## Journal Article Framing

**Target journals**: ACM TOCE, CSE, ICER special issues

**Article structure**:

### Introduction
- CER needs measurement infrastructure but lacks systematic methodologies
- Existing tools either ad-hoc or opaque about design process
- TCER provides framework but needs Phase 4.A* methodology examples

### Related Work
- Notional machines (Sorva, du Boulay, Ben-Ari)
- Educational tool development (Guo, PythonTutor; Kölling, BlueJ)
- Translational research in education (medical model, DBER)
- TCER framework (Cole et al., 2023)

### Theory-to-Requirements Methodology
- 5-step process: Literature → Taxonomies → Integration → Use Cases → Requirements
- Maps to TCER phases: 3.A → 3.B* → 1.A* → 1.B → 4.A*
- Bidirectional translation: forward (theory→tool) and reverse (use→theory)

### Application: Embody Tracer
- Case study demonstrating methodology
- Tool-theory co-evolution in practice
- Neutral infrastructure principle
- Trading zone coordination

### Research Contributions
1. 12 JavaScript notional machines (theoretical)
2. Systematic literature synthesis + practitioner guides (synthesizing)
3. Replicable methodology (methodological)
4. Working infrastructure prototype (infrastructural)

### Discussion
- Multi-phase nature as strength, not confusion
- Infrastructure as research contribution
- Generalizability to other domains
- Limitations and future work

### Conclusion
- Pattern for systematic infrastructure research
- Demonstrates TCER Phase 4.A* in practice
- Invites replication and adaptation

---

## Conference Talk Structure

**15-minute talk outline**:

### Slide 1: Title
- "Translational Infrastructure Research: Developing Embody Through Systematic Theory-to-Requirements Translation"
- Your name, institution, TCER 2025

### Slide 2: The Problem
- CER needs measurement infrastructure
- How do you develop tools systematically?
- Gap: No established Phase 4.A* methodology

### Slide 3: The Approach
- Theory-to-requirements: 5-step process
- Embody case study: JavaScript execution tracer
- Three contribution types

### Slide 4: Multi-Phase Nature
- Diagram: Phases 1.B, 3.A, 3.B*, 4.A* simultaneously
- NOT sequential—co-evolutionary
- This is what translational work looks like

### Slide 5: Contribution 1—Theories
- 12 JavaScript-specific notional machines
- Domain-specific, not borrowed
- Phase 1.B research contribution

### Slide 6: Contribution 2—Synthesis
- Literature review (3.A): 30+ papers
- Practitioner guides (3.B*): Tool dev taxonomies
- Trading zone function

### Slide 7: Contribution 3—Methodology
- Theory-to-requirements process
- Replicable pattern
- Maps to TCER phases

### Slide 8: Tool-Theory Co-Evolution
- Couldn't build without theories
- Couldn't test theories without tool
- Physics analogy: CERN :: Embody

### Slide 9: Neutral Infrastructure Principle
- Theory-neutral design
- Enables trading zones
- Serves diverse pedagogies

### Slide 10: Embody Demo
- Live trace output
- Configuration options
- "What tools do" vs. "What Embody enables"

### Slide 11: What This Demonstrates
- Infrastructure IS research when systematic
- Multi-phase presence expected in translational work
- Methodology fills CER gap

### Slide 12: Generalization
- Pattern applies to: type systems, collaboration, debugging
- Any domain requiring measurement infrastructure
- Replicable 5-step process

### Slide 13: Future Work
- Phase 2: Experiments using Embody
- Phase 4.B: Deployment feedback
- Phase 5: Scaled interventions
- Refinement cycle back to theories

### Slide 14: Key Takeaways
1. Infrastructure research needs systematic methodology
2. Tool-theory co-evolution is fundamental
3. Multi-phase work exemplifies translational spectrum
4. Pattern is replicable for other domains

### Slide 15: Questions
- Thank you
- Contact info
- Link to documentation

**Anticipated questions**:

**Q**: "Is this really Phase 1.B if the notional machines aren't empirically validated?"
**A**: Phase 1.B includes systematic extrapolation with documented confidence levels. We have 2 validated, 10 extrapolated—this is standard domain-specific theory development. Full validation is future Phase 2 work.

**Q**: "Couldn't you just call this 'tool development'?"
**A**: Traditional framing undervalues three contributions: theoretical (notional machines), synthesizing (lit review + guides), methodological (replicable process). TCER framing accurately captures research nature.

**Q**: "How do you know other projects can use your methodology?"
**A**: Theory-to-requirements pattern generalizes: any infrastructure requiring (1) domain theory + (2) systematic requirements + (3) trading zone coordination. Type systems, collaboration tools, debugging support all fit.

**Q**: "What's the advantage of TCER framing over Design-Based Research?"
**A**: DBR focuses on interventions (curricula, pedagogies). TCER explicitly accommodates infrastructure (Phase 4.A*) and trading zones. Both valuable—TCER better fit for measurement tools.

---

## Elevator Pitch (30 seconds)

"I developed Embody, a JavaScript execution tracer, through translational research that created three things: domain-specific notional machine theories, systematic practitioner guidelines, and a replicable methodology. The key insight is that building measurement infrastructure IS research when you do it systematically—like CERN building particle accelerators is physics research. My work shows how to develop educational tools through theory-to-requirements translation, operating in trading zones between researchers and practitioners."

---

## Contribution Claims with Evidence

**When writing about contributions, always pair claim with evidence pointer:**

### Theoretical Contribution (Phase 1.B)
**Claim**: "We developed 12 JavaScript-specific notional machines"
**Evidence**: `/theory-to-requirements/4-notional-machines/javascript-notional-machines/`
**Nature**: Domain-specific theories, systematically extrapolated from established NM research
**Impact**: Provides theoretical foundation for trace requirements

### Synthesizing Contribution (Phase 3.A)
**Claim**: "We conducted systematic literature synthesis"
**Evidence**: `/theory-to-requirements/0-literature-review/`
**Nature**: 30+ papers with evidence hierarchy, cross-paper analysis
**Impact**: Establishes research foundation, contributes to State of the Art

### Synthesizing Contribution (Phase 3.B*)
**Claim**: "We created practitioner-facing taxonomy guides"
**Evidence**: `/theory-to-requirements/1-taxonomies/`
**Nature**: Tool developer guides, use cases, config-to-pedagogy mappings
**Impact**: Trading zone between researchers and practitioners

### Methodological Contribution
**Claim**: "We developed replicable theory-to-requirements methodology"
**Evidence**: `/theory-to-requirements/README.md` methodology section
**Nature**: 5-step process mapping to TCER phases, documented for replication
**Impact**: Fills CER gap in Phase 4.A* methodologies

### Infrastructural Contribution (Phase 4.A*)
**Claim**: "We built evidence-based prototype tracer"
**Evidence**: Embody codebase + `/theory-to-requirements/` documentation
**Nature**: Research-grade tool, theory-neutral, trading zone coordinator
**Impact**: Enables future Phase 2 experiments and Phase 5 interventions

---

## Common Misconceptions to Address

### Misconception 1: "This is just tool development"
**Correction**: Tool development without theory creation, systematic process, or methodological contribution is implementation. Embody includes all three.

### Misconception 2: "You're in too many phases—pick one"
**Correction**: Multi-phase presence is expected for translational work. TCER paper explicitly shows bidirectional arrows and "translational spectrum." Embody demonstrates this.

### Misconception 3: "Infrastructure isn't research contribution"
**Correction**: Building measurement tools IS research when: (1) grounded in theory, (2) creates new theoretical knowledge, (3) enables research that wasn't feasible. See physics accelerators, biology microscopes.

### Misconception 4: "You need empirical validation for Phase 1.B"
**Correction**: Phase 1.B includes systematic theory development. Empirical validation is Phase 2.B. Domain-specific theories can be extrapolated with documented confidence—standard practice (cite Sorva's original NM work).

### Misconception 5: "Trading zones are just collaboration"
**Correction**: Trading zones specifically enable local coordination without global alignment. Key: diverse stakeholders use same infrastructure for different purposes without needing to agree on pedagogy.

---

## Key Phrases for Different Audiences

### For CER Researchers
- "Translational computing education research"
- "Phase 1.B domain-specific theories"
- "Trading zone coordination"
- "Systematic literature synthesis"
- "Theory-to-requirements methodology"

### For Educational Tool Developers
- "Neutral infrastructure principle"
- "Evidence-based trace requirements"
- "Configurable execution data"
- "Practitioner-facing guidelines"
- "Tool-theory co-evolution"

### For Funding Agencies
- "Replicable methodology for infrastructure research"
- "Fills methodological gap in CER"
- "Enables future empirical work"
- "Multi-institution coordination potential"
- "Scalable to other domains"

### For General Academic Audience
- "Like CERN for programming education"
- "Measurement infrastructure as research"
- "Bridges research and practice"
- "Systematic approach to tool development"
- "Three types of contribution"

---

## Summary

**Use this guide when**:
- Writing thesis chapters
- Submitting conference papers
- Preparing talks
- Responding to reviewers
- Applying for positions/funding

**Don't use in**:
- Technical documentation (theory-to-requirements)
- Tool developer guides
- User-facing materials
- Code comments

**The distinction**: Technical docs show through doing. Publication framing tells through explaining. Both are needed, but for different audiences.

---

**See also**:
- [Documentation Implications](../5-documentation-implications/) - How we integrated TCER into theory-to-requirements
- [Quick Reference](./README.md#b-quick-reference) - TCER terminology and citations
- [Reflexive Analysis](../6-reflexive-analysis/) - What we learned through this process

# Assessment Use Cases ‚Üí Trace Events Mapping (Phase 1B)

**Purpose**: Map all 50 assessment use cases to specific trace events and configurations

**Team**: All 3 personas collaborating - Educator verifies pedagogy, Developer maps events, Researcher checks boundary

**Status**: IN PROGRESS

---

## Mapping Template (for consistency)

```markdown
### [Use Case Name]
**Category**: [1-10]
**What tools do**: [From Step 4 table]
**Observable execution patterns**: [What in trace reveals this?]

**Required events**:
- [ ] `event.type`: [Why needed - what does it reveal?]

**Config settings**:
```javascript
{
  granularity: { ... },
  filters: { ... }
}
```

**Boundary compliance**:
- ‚úÖ Specifies WHAT data available
- ‚úÖ States HOW tools consume
- ‚ùå Does NOT specify algorithm
- ‚ùå Does NOT prescribe feedback

**Gaps**: [Missing events/config?]
```

---

## Category 1: Formative Assessment (7 use cases)

### Educator: "These need real-time or near-real-time feedback during practice"
### Developer: "So we need to balance detail with performance"
### Researcher: "And maintain formative boundary - no grading, just learning support"

---

### 1.1: Real-time syntax error detection

**Category**: 1 (Formative)
**What tools do**: Flag parse errors as students type with immediate contextual explanations

**Observable execution patterns**:
- **NONE** - This is PRE-trace (static analysis)
- Parser errors occur before code can execute

**Developer**: "Embody doesn't parse - tools do that before calling embody"
**Educator**: "Right, this is IDE/editor territory, not execution trace"
**Researcher**: "Document this as out-of-scope for trace-based assessment"

**Required events**:
- [ ] **NONE** - Static analysis, not execution trace

**Config settings**:
```javascript
// N/A - Not applicable to embody
```

**Boundary compliance**:
- ‚úÖ Correctly identifies scope boundary (static vs runtime)
- ‚úÖ Embody provides execution data, not parse data

**Gaps**: None - correctly out of scope

**Scope note**: Syntax errors prevent execution. Tools should parse before calling `embody()`.

---

### 1.2: Misconception-triggered feedback

**Category**: 1 (Formative)
**What tools do**: Detect execution patterns indicating specific misconceptions (var/let confusion, coercion errors, reference bugs, closure capture mistakes), provide targeted explanations

**Observable execution patterns**:
- **Hoisting/TDZ**: Variable read before declaration (TDZ access)
- **Type coercion**: Operators causing type conversion (== vs ===, + with strings)
- **Closure+loop**: Loop variable captured by closure (classic bug)
- **Reference vs value**: Object mutation through reference
- **Async timing**: Event loop ordering misunderstandings
- **Prototype lookup**: Inheritance chain confusion

**Educator**: "These are the big CS1/CS2 misconceptions we need to catch"
**Developer**: "TDZ is straightforward - `variable.tdz-access` event. Coercion needs expressions - that's expensive"
**Researcher**: "Qian & Lehman 2017 documented 9 misconception types - we should map to those"

**Team discussion excerpt**:
```
Researcher: "For coercion detection, we need `expression.binary` with `coercionOccurred` flag"
Developer: "But that's 3x overhead. Can we avoid it?"
Educator: "Type coercion is one of the highest-frequency misconceptions (65% of students). It's worth the cost for diagnostic contexts"
Developer: "Agreed, but we document this is for diagnostic assessment, NOT continuous real-time"
‚Üí Consensus: Include expressions with performance caveat
```

**Required events**:
- [x] `variable.tdz-access`: Hoisting misconception (has `inTDZ` flag)
- [x] `variable.declare`: Declaration sequence for hoisting understanding
- [x] `expression.binary`: Type coercion detection (has `coercionOccurred` flag) **‚ö†Ô∏è HIGH OVERHEAD**
- [x] `closure.capture`: Closure creation (has `capturedInLoop` boolean for loop bug)
- [x] `variable.assign`: Reference vs value (object IDs for tracking)
- [x] `variable.update`: Mutation through references
- [x] `prototype.lookup`: Prototype chain confusion (has `lookupChain[]`, `foundAt`)
- [x] `microtask.queue` + `microtask.execute`: Event loop ordering
- [x] `await.before` + `await.after`: Async timing
- [x] `function.call`: this binding misconceptions (has `thisValue`, `callType`)

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",       // Need reads for TDZ
    functions: "all",       // this binding, hoisting
    scopes: "all",          // Scope chain misconceptions
    controlFlow: "detailed", // Context for bugs
    async: "all",           // Event loop full detail
    objects: "all",         // Prototype, reference tracking
    errors: "all",          // Error patterns
    expressions: true       // ‚ö†Ô∏è Coercion detection (3x overhead)
  },
  filters: {
    excludeBuiltins: true   // Focus on student code
  },
  serialization: {
    values: "shallow",      // Object IDs sufficient
    objects: { prototype: true },  // Need prototype chain
    circularRefs: "registry"       // Track references
  }
}
```

**Boundary compliance**:
- ‚úÖ Specifies events that reveal misconception patterns
- ‚úÖ Config provides data needed for detection
- ‚ùå Does NOT specify detection algorithms (e.g., how to identify "consistent TDZ pattern")
- ‚ùå Does NOT prescribe feedback content
- ‚úÖ "Tools consume events ‚Üí implement pattern matching ‚Üí generate targeted feedback"

**Gaps**: None - comprehensive event coverage

**Performance note**: High overhead due to `expressions: true`. Justified for:
- Diagnostic assessment (not continuous)
- Small scale (individual/classroom, not MOOC)
- High pedagogical value (type coercion is #1 misconception)

---

### 1.3: POE cycle support

**Category**: 1 (Formative)
**What tools do**: Enable predict-observe-explain workflow: record student prediction, execute code, compare predicted vs actual behavior, prompt explanation of discrepancy

**Observable execution patterns**:
- **Complete execution trace** - Whatever student is predicting (variables? order? output?)
- Need comprehensive events to capture actual behavior

**Educator**: "POE is powerful pedagogy - prediction forces explicit mental model"
**Developer**: "This needs comprehensive trace since we don't know what they're predicting"
**Researcher**: "White & Frederiksen 1998 - prediction errors reveal mental model gaps"

**Required events**:
- [x] **All event types** - Depends on prediction target
- [x] If predicting values: All variable events
- [x] If predicting order: Sequence IDs critical
- [x] If predicting output: Function returns, final values

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",           // If async code
    objects: "all",
    errors: "all",
    expressions: false      // Probably not needed for POE
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "deep",         // Need actual values for comparison
    maxDepth: 5
  },
  output: {
    ordering: "async-aware" // If predicting execution order
  }
}
```

**Boundary compliance**:
- ‚úÖ Provides complete trace for comparison
- ‚úÖ Tools implement prediction comparison logic
- ‚ùå Does NOT specify how to present discrepancies
- ‚ùå Does NOT prescribe explanation prompts

**Gaps**: None - comprehensive trace sufficient

---

### 1.4: Progressive hint scaffolding

**Category**: 1 (Formative)
**What tools do**: Provide multi-level hints progressing from conceptual ‚Üí strategic ‚Üí tactical ‚Üí code-specific based on student struggle duration

**Observable execution patterns**:
- **Error frequency**: Repeated errors indicate struggle
- **Time between events**: Long gaps = stuck
- **Error‚Üífix‚Üísame-error cycles**: Trial-and-error pattern

**Developer**: "This is mostly TEMPORAL analysis - error patterns over time"
**Educator**: "We need to see repeated attempts, not just single execution"
**Researcher**: "This crosses into development environment territory - we need multiple traces"

**Team discussion**:
```
Developer: "A single trace shows one execution. Scaffolding needs longitudinal data"
Educator: "Right - we need to track across multiple attempts"
Researcher: "So embody provides: error events per execution. Tools track: student history across executions"
‚Üí Consensus: Error events enable detection, but scaffolding logic is tool-side + multi-trace
```

**Required events**:
- [x] `error.throw`: Error occurrence
- [x] `error.context`: Stack trace for locating problem
- [x] Contextual events around error (variables, functions leading to error)

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",       // Context for error
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",           // If async errors
    objects: "all",
    errors: "all",          // Critical
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow"
  }
}
```

**Boundary compliance**:
- ‚úÖ Single trace provides: error location + context
- ‚úÖ Tools implement: longitudinal tracking, hint progression logic, struggle detection
- ‚ùå Does NOT specify hint content or progression strategy

**Gaps**: None - error events + context sufficient per trace

**Scope note**: Hint scaffolding requires MULTIPLE traces (student history). Embody provides per-execution data; tools track across executions.

---

### 1.5: Live coding quality feedback

**Category**: 1 (Formative)
**What tools do**: Analyze code during development, provide immediate feedback on quality dimensions (naming, API usage, structure, conventions) beyond correctness

**Observable execution patterns**:
- **Naming**: Variable/function identifiers
- **API choices**: Which library functions called
- **Structure**: Nesting depth, function boundaries
- **Conventions**: Naming style (camelCase, etc.)

**Educator**: "Real-time quality feedback builds good habits early"
**Developer**: "This is QLCs but real-time - so we need minimal overhead"
**Researcher**: "Lehtinen 2023 QLCs - but they did batch processing. Real-time needs performance"

**Required events**:
- [x] `variable.declare`: Identifier names
- [x] `function.call`: API usage, function names
- [x] `conditional.branch`: Nesting/complexity proxy
- [x] `loop.enter`: Loop patterns

**Config settings**:
```javascript
{
  granularity: {
    variables: "declarations-only",  // Just names (minimal)
    functions: "user-code-only",     // API + student functions
    scopes: "none",                  // Not needed for quality
    controlFlow: "detailed",         // Structure/algorithms
    async: "await-only",             // Basic async pattern
    objects: "creation-only",        // Design patterns
    errors: "throws-only",
    expressions: false               // Too expensive for real-time
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow",
    maxDepth: 1
  },
  performance: {
    streaming: true  // Real-time feedback
  }
}
```

**Boundary compliance**:
- ‚úÖ Minimal events for performance (real-time constraint)
- ‚úÖ Tools implement quality scoring, naming rubrics
- ‚ùå Does NOT specify quality criteria

**Gaps**: None - minimal QLCs events sufficient

**Performance note**: Optimized for real-time - no expressions, declarations-only, streaming

---

### 1.6: Liminal state scaffolding

**Category**: 1 (Formative)
**What tools do**: Detect threshold liminal state indicators (inconsistent predictions, partial correctness, prolonged struggle), increase scaffolding frequency and specificity

**Observable execution patterns**:
- **Inconsistent behavior across attempts**: Sometimes correct, sometimes not
- **Partial correctness**: Some test cases pass, others fail
- **Pattern analysis**: Needs multiple traces to detect inconsistency

**Educator**: "Liminal state is the 'troublesome' learning zone - students are confused but progressing"
**Researcher**: "Meyer & Land 2003 - threshold concepts have liminal phase. Need longitudinal data"
**Developer**: "This is similar to progressive scaffolding - needs multi-trace analysis"

**Required events**:
- [x] **Comprehensive trace per attempt** - To assess correctness patterns
- [x] Threshold-specific events:
  - Closures: `closure.capture`, `scope.*`
  - Async: `microtask.*`, `await.*`
  - Prototypes: `prototype.lookup`
  - Recursion: `function.call/return` patterns

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",          // Closure threshold
    controlFlow: "detailed",
    async: "all",           // Async threshold
    objects: "all",         // Prototype threshold
    errors: "all",
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow",
    maxDepth: 2
  }
}
```

**Boundary compliance**:
- ‚úÖ Comprehensive trace per attempt
- ‚úÖ Tools implement: longitudinal analysis, liminal state classification, scaffolding adaptation
- ‚ùå Does NOT specify liminal state criteria

**Gaps**: None

**Scope note**: Liminal state detection requires MULTIPLE traces. Embody provides comprehensive per-trace data; tools track patterns across attempts.

---

### 1.7: Integrated concept practice feedback

**Category**: 1 (Formative)
**What tools do**: Present tasks requiring coordination of multiple concepts (closure + async, NM integration), provide formative feedback on connections and interactions

**Observable execution patterns**:
- **Concept A + Concept B usage**: E.g., closures capturing async state
- **Integration points**: Where concepts interact in execution

**Educator**: "Integration is harder than isolated concepts - students struggle with combinations"
**Developer**: "Need events from both concepts"
**Researcher**: "Framework application - we know individual concepts work, testing integration"

**Required events**:
- [x] **Depends on concepts being integrated**
- [x] Closure + async example:
  - `closure.capture` (closure concept)
  - `microtask.queue/execute` (async concept)
  - `scope.*` (shared by both)
  - Sequence IDs (timing across concepts)

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",          // Integration point
    controlFlow: "detailed",
    async: "all",           // If async involved
    objects: "all",         // If objects involved
    errors: "all",
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow"
  },
  output: {
    ordering: "async-aware"  // If async+other concept
  }
}
```

**Boundary compliance**:
- ‚úÖ Provides events from multiple concepts
- ‚úÖ Tools detect integration patterns, provide feedback
- ‚ùå Does NOT specify what counts as "correct integration"

**Gaps**: None - concept-specific events already defined

---

## Phase 1B - Category 1 Team Review

**Educator**: "All 7 formative use cases mapped. Most need comprehensive events since formative is about rich feedback"
**Developer**: "Performance concerns for #2 (expressions) and #5 (real-time). Documented appropriately"
**Researcher**: "Three use cases (#4, #6, #7) need multiple traces for longitudinal analysis - documented scope boundaries"

**Quality check**:
- [x] All 7 use cases have event mappings ‚úÖ
- [x] Observable patterns identified ‚úÖ
- [x] Configs specified with rationale ‚úÖ
- [x] Boundary compliance verified ‚úÖ
- [x] Performance implications noted ‚úÖ
- [x] Scope boundaries documented (static analysis, multi-trace) ‚úÖ

**Gaps identified**: None for Category 1

**Move to Category 2** ‚úÖ

---

## Category 2: Summative Assessment (5 use cases)

### Educator: "These are graded - need reliable, objective data"
### Developer: "And often at scale - performance matters"
### Researcher: "Evidence of achievement, not just learning support"

---

### 2.1: Quality-based grading (QLCs)

**Category**: 2 (Summative)
**What tools do**: Assess code quality dimensions (naming descriptiveness and conventions, API usage appropriateness and modernity, algorithmic approach efficiency and clarity, design pattern recognition) at scale beyond functional correctness

**Observable execution patterns**:
- **Naming**: Variable/function names from declarations
- **API**: Which functions called (modern array methods vs manual loops)
- **Algorithms**: Loop patterns (nested vs linear), recursion vs iteration
- **Design patterns**: Object creation, closure usage, functional patterns

**Educator**: "This is Lehtinen et al. 2023 - validated at MOOC scale (thousands of students)"
**Developer**: "Performance critical since it's at scale. Minimal events"
**Researcher**: "üî¨ Established evidence - QLCs are validated, trace-based implementation is our contribution"

**Required events**:
- [x] `variable.declare`: Names for quality assessment
- [x] `function.call`: API choices, function names
- [x] `conditional.branch`: Decision complexity
- [x] `loop.*`: Iteration patterns, nesting
- [x] `object.create`: Design patterns
- [x] `closure.capture`: Functional patterns

**Config settings**:
```javascript
{
  granularity: {
    variables: "declarations-only",  // Just names (efficient)
    functions: "user-code-only",     // Student API choices
    scopes: "none",                  // Not needed for quality
    controlFlow: "detailed",         // Algorithms critical
    async: "await-only",             // Basic async pattern
    objects: "creation-only",        // Design patterns
    errors: "throws-only",
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow",
    functions: { name: true, location: true }
  },
  performance: {
    maxEvents: 10000  // Reasonable limit for student code
  }
}
```

**Boundary compliance**:
- ‚úÖ Provides naming, API, algorithm, structure data
- ‚úÖ Tools implement quality scoring rubrics
- ‚ùå Does NOT specify scoring criteria or thresholds

**Gaps**: None

**Performance note**: Optimized for scale - minimal overhead, still comprehensive for quality

---

### 2.2: Taxonomy-aligned progression rubrics

**Category**: 2 (Summative)
**What tools do**: Evaluate student work against learning progression models (SOLO levels from isolated facts to integrated understanding to transfer, Block Model phases, BSI dimensions)

**Observable execution patterns**:
- **Trace complexity**: Simple (isolated operations) vs complex (integrated patterns)
- **SOLO levels** via execution:
  - Unistructural: Single operation
  - Multistructural: Multiple operations, unconnected
  - Relational: Operations connected (e.g., function composition)
  - Extended Abstract: Transfer to new context

**Educator**: "Taxonomy classification needs holistic trace analysis - what patterns appear"
**Developer**: "So comprehensive trace, tools do the complexity classification"
**Researcher**: "üìê Framework - taxonomies are established (Biggs & Collis 1982), mapping to trace is our application"

**Required events**:
- [x] **Comprehensive coverage** - All event types to assess complexity
- [x] Variables, functions, scopes, control flow, objects

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: false  // Not needed for taxonomy
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow"
  }
}
```

**Boundary compliance**:
- ‚úÖ Comprehensive trace for complexity analysis
- ‚úÖ Tools implement taxonomy classification
- ‚ùå Does NOT specify taxonomy criteria

**Gaps**: None

---

### 2.3: Multi-dimensional evaluation

**Category**: 2 (Summative)
**What tools do**: Score correctness (tests pass), quality (professional standards), comprehension (can explain), and process (debugging/development approach) separately with weighted rubric

**Observable execution patterns**:
- **Correctness**: Execution succeeds, expected outputs
- **Quality**: QLCs dimensions (naming, API, structure)
- **Comprehension**: NOT in trace (requires explanation)
- **Process**: NOT in single trace (requires development history)

**Team discussion**:
```
Educator: "Process and comprehension are outside single trace scope"
Developer: "Correctness = no errors + correct output. Quality = QLCs events"
Researcher: "Two dimensions trace-based, two require other data sources"
‚Üí Consensus: Specify what embody provides (correctness + quality data), note comprehension/process limitations
```

**Required events**:
- [x] **For correctness**: Error events, function returns (outputs)
- [x] **For quality**: QLCs events (see 2.1)
- [ ] **For comprehension**: NOT in trace (verbal explanation needed)
- [ ] **For process**: NOT in single trace (development history needed)

**Config settings**:
```javascript
{
  // Combine correctness + quality needs
  granularity: {
    variables: "declarations-only",  // Quality (naming)
    functions: "user-code-only",
    scopes: "none",
    controlFlow: "detailed",         // Quality (algorithms)
    async: "await-only",
    objects: "creation-only",
    errors: "all",                   // Correctness
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow"
  }
}
```

**Boundary compliance**:
- ‚úÖ Provides correctness data (errors, outputs)
- ‚úÖ Provides quality data (QLCs dimensions)
- ‚úÖ Notes comprehension requires verbal assessment (out of scope)
- ‚úÖ Notes process requires development history (multi-trace)

**Gaps**: None - correctly scoped

**Scope note**: 2 of 4 dimensions are trace-based. Comprehension and process require additional data sources.

---

### 2.4: Threshold crossing verification

**Category**: 2 (Summative)
**What tools do**: Require demonstration of threshold concept mastery (async event loop understanding through complex scenario prediction, closure mechanism through design tasks, prototype delegation, recursion)

**Observable execution patterns**:
- **Per threshold**:
  - Async: Microtask ordering correct, promise handling
  - Closures: Correct capture, scope chain understanding
  - Prototypes: Delegation chain correct
  - Recursion: Base case, recursive call patterns

**Educator**: "Post-threshold = can use concept in complex scenarios"
**Developer**: "Threshold-specific events - closure events for closures, async for async, etc."
**Researcher**: "üìê Framework - thresholds established (Meyer & Land 2003), verification via trace is application"

**Required events**:
- [x] **Async threshold**: `microtask.*`, `await.*`, `promise.*`
- [x] **Closure threshold**: `closure.capture`, `scope.*`, `variable.read` (with `resolvedScopeId`)
- [x] **Prototype threshold**: `prototype.lookup`, `object.create`
- [x] **Recursion threshold**: `function.call/return` (recursive patterns)

**Config settings**:
```javascript
{
  // Focused on threshold-relevant events
  granularity: {
    variables: "all",       // Closure threshold
    functions: "all",       // Recursion, call patterns
    scopes: "all",          // Closure threshold
    controlFlow: "detailed", // Recursion structure
    async: "all",           // Async threshold
    objects: "all",         // Prototype threshold
    errors: "all",
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow",
    objects: { prototype: true }  // Prototype threshold
  },
  output: {
    ordering: "async-aware"  // Async threshold critical
  }
}
```

**Boundary compliance**:
- ‚úÖ Provides threshold-specific event data
- ‚úÖ Tools verify correct usage patterns
- ‚ùå Does NOT specify what constitutes "mastery"

**Gaps**: None

---

### 2.5: Integration task assessment

**Category**: 2 (Summative)
**What tools do**: Assess application of multiple concepts together in realistic complex tasks (async data processor using closures for state management, recursive tree traversal with prototype methods)

**Observable execution patterns**:
- **Multiple concepts interacting**: Closure + async, recursion + prototypes, etc.
- **Integration points**: Where concepts coordinate

**Educator**: "This is the 'can you build something real' test"
**Developer**: "Comprehensive trace - we don't know which concepts will combine"
**Researcher**: "Similar to formative #7 but graded, higher stakes"

**Required events**:
- [x] **Comprehensive** - All concept events might be relevant

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: false
  },
  filters: {
    excludeBuiltins: true
  },
  serialization: {
    values: "shallow",
    maxDepth: 3  // Nested structures in complex tasks
  },
  output: {
    ordering: "async-aware"
  }
}
```

**Boundary compliance**:
- ‚úÖ Comprehensive multi-concept data
- ‚úÖ Tools assess integration quality
- ‚ùå Does NOT specify integration criteria

**Gaps**: None

---

## Phase 1B - Category 2 Team Review

**Educator**: "All 5 summative use cases mapped. Most need comprehensive or QLCs-focused events"
**Developer**: "#1 (QLCs) is most performance-sensitive - optimized appropriately. Others are comprehensive"
**Researcher**: "Good scope boundaries on #3 (comprehension/process outside trace). Evidence classifications maintained"

**Quality check**:
- [x] All 5 use cases mapped ‚úÖ
- [x] Configs balance detail vs performance ‚úÖ
- [x] Boundary compliance verified ‚úÖ
- [x] Scope limitations documented (#3) ‚úÖ

**Gaps**: None for Category 2

**Move to Category 3** ‚úÖ

---

## Category 3: Diagnostic Assessment (6 use cases)

### Educator: "Goal is identifying specific gaps before intervention"
### Developer: "Similar events to formative but focused on detection, not continuous feedback"
### Researcher: "Implicit assessment - detect from normal practice, not explicit tests"

---

### 3.1: Misconception pattern detection

**Category**: 3 (Diagnostic)
**What tools do**: Analyze execution patterns during normal coding practice to detect specific misconception signatures (uninitialized variable reads for auto-init misconception, closure+loop+var pattern, async timing violations)

**Observable execution patterns**:
**Same as 1.2 (Misconception-triggered feedback)** but passive observation vs immediate feedback

**Developer**: "Same events as formative #2, different tool usage (batch analysis vs real-time)"
**Educator**: "Diagnostic = detect silently, intervene strategically. Formative = immediate feedback"
**Researcher**: "Boundary: same data, different pedagogical application"

**Required events**:
- [x] **Same as 1.2**: TDZ, coercion, closure+loop, prototype, async, reference

**Config settings**:
```javascript
// Same as 1.2 - comprehensive misconception detection
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: true  // ‚ö†Ô∏è Coercion detection
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow", objects: { prototype: true }, circularRefs: "registry" }
}
```

**Boundary compliance**:
- ‚úÖ Same events, different tool behavior (passive vs active)
- ‚úÖ Tools decide when to intervene vs silently track

**Gaps**: None

**Note**: Same trace data as formative misconception feedback - difference is pedagogical strategy (when/how to provide feedback)

---

### 3.2: Pre-assessment readiness testing

**Category**: 3 (Diagnostic)
**What tools do**: Identify existing misconceptions before instruction begins through prediction tasks targeting known error patterns

**Observable execution patterns**:
- **Prediction vs actual**: Compare student prediction to trace
- **Error patterns**: Specific misconceptions revealed by prediction errors

**Educator**: "Diagnostic pre-test - what do students already misunderstand"
**Developer**: "This is like POE (#1.3) but diagnostic purpose"
**Researcher**: "Prior knowledge assessment - prevents mis-attributing learning to instruction"

**Required events**:
- [x] **Comprehensive trace** for prediction comparison
- [x] **Misconception-specific events** for pattern detection

**Config settings**:
```javascript
{
  // Comprehensive for prediction comparison
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: true  // If testing coercion understanding
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "deep", maxDepth: 5 },  // Need actual values
  output: { ordering: "async-aware" }
}
```

**Boundary compliance**:
- ‚úÖ Comprehensive trace for comparison
- ‚úÖ Tools compare prediction to actual, classify misconceptions
- ‚ùå Does NOT specify prediction interface or comparison algorithm

**Gaps**: None

---

### 3.3: Threshold state detection

**Category**: 3 (Diagnostic)
**What tools do**: Distinguish pre-threshold (no systematic model), liminal (inconsistent predictions, partial understanding), and post-threshold (consistent accuracy, model-based explanations) states for each JavaScript threshold

**Observable execution patterns**:
- **Pattern consistency across attempts**: Consistent = post, inconsistent = liminal, absent = pre
- **Correctness patterns**: Partial = liminal, complete = post

**Educator**: "Liminal state is 'troublesome knowledge' - students are confused but progressing"
**Developer**: "Needs longitudinal data - multiple traces to assess consistency"
**Researcher**: "Meyer & Land 2003 - explicit threshold state model"

**Required events**:
- [x] **Threshold-specific events** (same as 2.4)
- [x] Multiple traces to assess consistency

**Config settings**:
```javascript
{
  // Focused on threshold events
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",       // Closures
    controlFlow: "detailed",
    async: "all",        // Async threshold
    objects: "all",      // Prototypes
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow", objects: { prototype: true } },
  output: { ordering: "async-aware" }
}
```

**Boundary compliance**:
- ‚úÖ Per-trace threshold data
- ‚úÖ Tools classify state across multiple traces
- ‚ùå Does NOT specify state classification criteria

**Gaps**: None

**Scope note**: Requires multiple traces for state classification (consistency analysis)

---

### 3.4: Taxonomy level diagnosis

**Category**: 3 (Diagnostic)
**What tools do**: Determine student's current SOLO level, Block Model level, or BSI dimension mastery to assign appropriately challenging tasks

**Observable execution patterns**:
- **Trace complexity**: Proxy for taxonomy level
- **Same as 2.2** but diagnostic purpose (inform instruction vs grade)

**Educator**: "Diagnose to match task difficulty, prevent ceiling/floor effects"
**Developer**: "Same comprehensive trace as summative taxonomy (#2.2)"
**Researcher**: "Same data, different pedagogical purpose"

**Required events**:
- [x] **Comprehensive** (same as 2.2)

**Config settings**:
```javascript
// Same as 2.2 - comprehensive for complexity analysis
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Same data as summative taxonomy
- ‚úÖ Tools use for diagnostic vs grading purpose

**Gaps**: None

---

### 3.5: Notional machine understanding diagnosis

**Category**: 3 (Diagnostic)
**What tools do**: When errors occur, identify which of 12 JavaScript notional machines student misunderstands (memory model, scope chain, event loop, call stack) through pattern analysis

**Observable execution patterns**:
- **NM-specific patterns**:
  - Memory model: Variable lifecycles, object references
  - Scope chain: Variable resolution (`resolvedScopeId`)
  - Event loop: Microtask ordering, async timing
  - Call stack: Function call/return sequences
  - Etc. for other NMs

**Educator**: "Sorva 2013 - 12 NMs for JavaScript. Diagnose which student misunderstands"
**Developer**: "Different NMs need different events - comprehensive coverage needed"
**Researcher**: "üìê Framework - NMs established, trace-based diagnosis is application"

**Required events**:
- [x] **NM-specific events**:
  - Memory: `variable.*`, `object.*`
  - Scope chain: `scope.*`, `variable.read` (with `resolvedScopeId`)
  - Event loop: `microtask.*`, `await.*`
  - Call stack: `function.call/return`
  - Prototype chain: `prototype.lookup`

**Config settings**:
```javascript
{
  // Comprehensive - cover all NMs
  granularity: {
    variables: "all",       // Memory, scope
    functions: "all",       // Call stack
    scopes: "all",          // Scope chain
    controlFlow: "detailed",
    async: "all",           // Event loop
    objects: "all",         // Prototype chain, memory
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow", objects: { prototype: true }, circularRefs: "registry" }
}
```

**Boundary compliance**:
- ‚úÖ Comprehensive NM-relevant events
- ‚úÖ Tools diagnose which NM is misunderstood
- ‚ùå Does NOT specify diagnostic algorithm

**Gaps**: None - NM diversity requires comprehensive events

---

### 3.6: Prerequisite gap identification

**Category**: 3 (Diagnostic)
**What tools do**: Before threshold concept instruction, verify prerequisites met (scope understanding before closures, callback competence before async, object basics before prototypes, call stack model before recursion)

**Observable execution patterns**:
- **Prerequisite-specific**:
  - Scope (before closures): Scope chain traversal correct
  - Callbacks (before async): Function-as-value usage
  - Objects (before prototypes): Object creation/access
  - Call stack (before recursion): Function call/return understanding

**Educator**: "Can't learn X without prerequisite Y - diagnose Y first"
**Developer**: "Targeted events per prerequisite - don't need comprehensive"
**Researcher**: "Scaffolding principle - build on solid foundation"

**Required events**:
- [x] **Depends on prerequisite being tested**:
  - Scope prerequisite: `scope.*`, `variable.*`
  - Callback prerequisite: `function.call` (functions as values)
  - Object prerequisite: `object.*`, `property.access`
  - Call stack prerequisite: `function.call/return`

**Config settings**:
```javascript
{
  // Example: Testing scope understanding (before closures)
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",       // Focus
    controlFlow: "basic",
    async: "none",       // Not relevant for scope prerequisite
    objects: "none",
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Prerequisite-specific events
- ‚úÖ Tools diagnose prerequisite understanding
- ‚ùå Does NOT specify prerequisite criteria

**Gaps**: None - prerequisite-specific configs

**Note**: Config varies by prerequisite - example shows scope testing

---

## Phase 1B - Category 3 Team Review

**Educator**: "All 6 diagnostic use cases mapped. Several overlap with formative/summative (same data, different purpose)"
**Developer**: "3 use cases need longitudinal data (#3.3). Most need comprehensive events since we're detecting subtle patterns"
**Researcher**: "Good boundary clarity - same events can serve different pedagogical purposes"

**Quality check**:
- [x] All 6 diagnostic use cases mapped ‚úÖ
- [x] Overlaps with other categories noted (same data, different purpose) ‚úÖ
- [x] Longitudinal needs documented ‚úÖ
- [x] Boundary compliance verified ‚úÖ

**Gaps**: None for Category 3

**Move to Category 4** ‚úÖ

---

## Category 4: Quality Assessment / QLCs (6 use cases)

### Educator: "This is Lehtinen et al. 2023 - empirically validated at MOOC scale"
### Developer: "Performance critical - most will be 'declarations-only' and basic control flow"
### Researcher: "üî¨ Established - QLCs framework is validated, we're implementing via trace"

---

### 4.1: Identifier naming quality assessment

**Category**: 4 (QLCs)
**What tools do**: Evaluate variable/function/class names for descriptiveness, convention adherence, consistency, avoiding single letters except standard conventions

**Observable execution patterns**:
- **Variable names**: From `variable.declare` events
- **Function names**: From `function.call` events

**Team consensus**:
```
Developer: "Simplest QLC - just grab names from declarations"
Educator: "Naming is highest-impact QLC - students use terrible names"
Researcher: "Lehtinen 2023 Table 2 - naming quality strongly predicts code quality overall"
‚Üí Minimal config: declarations-only, very efficient
```

**Required events**:
- [x] `variable.declare`: Provides `variableName` field
- [x] `function.call`: Provides `functionName` field

**Config settings**:
```javascript
{
  granularity: {
    variables: "declarations-only",  // ‚úÖ Minimal, efficient
    functions: "user-code-only",     // Student function names
    scopes: "none",
    controlFlow: "none",             // Not needed for naming
    async: "none",
    objects: "none",
    errors: "none",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Provides identifier names
- ‚úÖ Tools implement naming quality rubrics (descriptiveness, conventions, consistency)
- ‚ùå Does NOT specify naming standards

**Gaps**: None

**Performance**: Minimal overhead - most efficient QLC

---

### 4.2: API usage appropriateness evaluation

**Category**: 4 (QLCs)
**What tools do**: Assess chosen APIs for task appropriateness (modern array methods vs manual loops, built-in functions vs reinventing wheels, security-conscious choices)

**Observable execution patterns**:
- **Which functions called**: Modern (Array.map) vs manual (for loops)
- **Built-in vs custom**: Using String.includes vs custom search

**Developer**: "Need to see built-in calls for this - so `functions: 'all'` not 'user-code-only'"
**Educator**: "Modern APIs are more expressive - we want students to learn them"
**Researcher**: "QLCs framework includes 'API usage' dimension explicitly"

**Required events**:
- [x] `function.call`: Provides `functionName`, `arguments`
- [x] Need built-in calls visible

**Config settings**:
```javascript
{
  granularity: {
    variables: "none",               // Not needed for API
    functions: "all",                // ‚úÖ Include built-ins
    scopes: "none",
    controlFlow: "none",
    async: "none",
    objects: "none",
    errors: "none",
    expressions: false
  },
  filters: {
    excludeBuiltins: false  // ‚ö†Ô∏è Opposite of usual - we WANT built-ins
  },
  serialization: {
    values: "shallow",
    functions: { name: true, location: true }
  }
}
```

**Boundary compliance**:
- ‚úÖ Provides API call data (which functions used)
- ‚úÖ Tools assess appropriateness (modern vs outdated, etc.)
- ‚ùå Does NOT specify API preferences

**Gaps**: None

**Note**: Unusual config - includes built-ins (opposite of most other use cases)

---

### 4.3: Algorithmic approach detection

**Category**: 4 (QLCs)
**What tools do**: Identify algorithm category (iteration vs recursion, linear vs nested loops, brute force vs optimization), estimate complexity class, recognize patterns

**Observable execution patterns**:
- **Iteration vs recursion**: Loop events vs recursive `function.call` patterns
- **Nesting**: Nested loop events (loop within loop)
- **Complexity**: Loop iteration counts, function call depth

**Developer**: "Need detailed control flow for algorithm structure"
**Educator**: "Students often choose inefficient algorithms - we want to guide them"
**Researcher**: "QLCs 'algorithmic approach' dimension"

**Required events**:
- [x] `loop.*`: Iteration patterns, nesting
- [x] `function.call/return`: Recursion detection (same `functionId` called within itself)
- [x] `conditional.branch`: Decision complexity

**Config settings**:
```javascript
{
  granularity: {
    variables: "none",
    functions: "all",               // Recursion detection
    scopes: "none",
    controlFlow: "detailed",        // ‚úÖ Critical - loop iterations, branches
    async: "none",
    objects: "none",
    errors: "none",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Provides algorithm structure data (loops, recursion, branches)
- ‚úÖ Tools classify algorithm type, estimate complexity
- ‚ùå Does NOT specify complexity criteria

**Gaps**: None

---

### 4.4: Design pattern recognition

**Category**: 4 (QLCs)
**What tools do**: Detect usage of professional patterns (factory, observer, module, singleton, accumulator, guard clause, early return) vs anti-patterns

**Observable execution patterns**:
- **Factory pattern**: Function returns objects (`function.return` with object)
- **Module pattern**: Closure capturing private variables
- **Observer pattern**: Function arrays, callback registration
- **Accumulator pattern**: Loop with aggregation

**Developer**: "This needs object creation + closures + function patterns"
**Educator**: "Design patterns are CS2/advanced CS1 - signal professional thinking"
**Researcher**: "Pattern recognition from execution - structural markers"

**Required events**:
- [x] `object.create`: Object construction patterns
- [x] `closure.capture`: Module pattern, private variables
- [x] `function.call`: Factory functions, observer callbacks
- [x] `variable.*`: Accumulator patterns (loop + variable update)

**Config settings**:
```javascript
{
  granularity: {
    variables: "writes-only",       // Accumulator pattern
    functions: "all",               // Factory, callbacks
    scopes: "function-only",        // Closure detection
    controlFlow: "basic",           // Loop boundaries for accumulator
    async: "none",
    objects: "creation-only",       // Object patterns
    errors: "none",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Provides pattern structure data
- ‚úÖ Tools recognize patterns
- ‚ùå Does NOT specify pattern taxonomy

**Gaps**: None

---

### 4.5: Code complexity measurement

**Category**: 4 (QLCs)
**What tools do**: Assess cyclomatic complexity (decision path count), nesting depth, function length, module coupling/cohesion

**Observable execution patterns**:
- **Cyclomatic complexity**: Count `conditional.branch` events
- **Nesting depth**: Nested loop/conditional levels
- **Function length**: Lines between `function.call` and `function.return` (approximate)

**Developer**: "Complexity metrics are objective counts - straightforward from events"
**Educator**: "'Simple is better' - teach students to reduce complexity"
**Researcher**: "Objective metrics (countable), not quality judgments"

**Required events**:
- [x] `conditional.branch`: Decision points (cyclomatic complexity)
- [x] `loop.enter`: Loop nesting
- [x] `function.call/return`: Function boundaries

**Config settings**:
```javascript
{
  granularity: {
    variables: "none",
    functions: "all",               // Function boundaries
    scopes: "none",
    controlFlow: "detailed",        // ‚úÖ Branches, loops
    async: "none",
    objects: "none",
    errors: "none",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Provides countable complexity data (branches, nesting, function counts)
- ‚úÖ Tools compute complexity metrics
- ‚ùå Does NOT specify complexity thresholds

**Gaps**: None

**Note**: Objective counts from events - tools compute metrics

---

### 4.6: Code structure evaluation

**Category**: 4 (QLCs)
**What tools do**: Assess organization (modularity, separation of concerns), clarity (self-documenting vs cryptic), maintainability (DRY adherence, coupling/cohesion balance)

**Observable execution patterns**:
- **Modularity**: Function decomposition (function count, call patterns)
- **Separation**: Function purpose clustering
- **DRY violations**: Repeated code patterns (similar event sequences)

**Educator**: "Structure is about architecture - harder to assess from trace alone"
**Developer**: "We can see function boundaries, calls - gives structural sketch"
**Researcher**: "Partial assessment - structure also requires static analysis (AST)"

**Required events**:
- [x] `function.call/return`: Function boundaries, modularity
- [x] `object.create`: Encapsulation
- [x] Control flow patterns: DRY detection (repeated sequences)

**Config settings**:
```javascript
{
  granularity: {
    variables: "declarations-only", // Variable organization
    functions: "all",               // Modularity
    scopes: "function-only",        // Encapsulation
    controlFlow: "basic",           // Pattern detection
    async: "none",
    objects: "creation-only",       // Encapsulation
    errors: "none",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Provides structural markers (functions, objects, patterns)
- ‚úÖ Tools assess structure quality
- ‚ùå Does NOT specify structure criteria

**Gaps**: Moderate - structure assessment benefits from static analysis (AST) in addition to trace. Trace provides execution structure, AST provides code structure.

**Scope note**: Trace gives execution view, AST gives code view. Combining both gives fuller picture.

---

## Phase 1B - Category 4 Team Review

**Educator**: "All 6 QLCs use cases mapped. These are the most performance-efficient configs"
**Developer**: "Mostly 'declarations-only' and 'detailed control flow' - minimal overhead. #4.2 is exception (needs built-ins)"
**Researcher**: "Strong evidence basis (üî¨) - QLCs validated at MOOC scale (Lehtinen 2023)"

**Quality check**:
- [x] All 6 QLCs mapped ‚úÖ
- [x] Configs optimized for performance ‚úÖ
- [x] Boundary compliance (objective data, tools assess quality) ‚úÖ
- [x] One gap noted (#4.6 benefits from AST + trace) ‚úÖ

**Gaps**: #4.6 (structure) benefits from static analysis in addition to trace - documented

**Move to Category 5** ‚úÖ

---

## Category 5: Process Assessment (4 use cases)

### Educator: "These assess HOW students develop, not just WHAT they produce"
### Developer: "Critical finding: MOST process assessment is outside single-trace scope"
### Researcher: "Development environment vs execution trace - important boundary"

---

### 5.1: Debugging strategy classification

**Category**: 5 (Process)
**What tools do**: Distinguish systematic hypothesis-driven debugging from trial-and-error random changes (print spam, uncommenting lines, guess-and-check)

**Observable execution patterns**:
- **Error‚Üífix cycles**: Repeated errors suggest trial-and-error
- **Systematic narrowing**: Targeted changes (hard to detect from trace)
- **Tool usage**: Debugger vs print spam (NOT in trace)

**Team discussion**:
```
Educator: "Systematic debugging is teachable skill - we want to assess it"
Developer: "But debugger usage isn't in execution trace - that's IDE/environment"
Researcher: "We can see error patterns (error‚Üífix‚Üíerror cycles) but not debugging process"
‚Üí Consensus: Error patterns detectable, but full debugging process needs environment data
```

**Required events**:
- [x] `error.throw`: Error occurrence
- [x] `error.context`: Stack traces
- [ ] **NOT in trace**: Debugger usage, breakpoints, variable inspection

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",       // Context around errors
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",           // If async errors
    objects: "all",
    errors: "all",          // ‚úÖ Critical
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Trace provides: Error occurrence patterns across attempts
- ‚úÖ Tools detect: Trial-and-error patterns (repeated errors)
- ‚ùå Trace does NOT provide: Debugger usage, code changes between attempts, hypothesis formation

**Gaps**: **MAJOR** - Full debugging process requires development environment data (IDE logs, code diffs, tool usage). Trace provides error patterns only.

**Scope note**: Process assessment largely outside embody scope. Embody provides error patterns; environment tracks debugging actions.

---

### 5.2: Development pattern analysis

**Category**: 5 (Process)
**What tools do**: Track whether student writes tests first or after implementation, uses version control checkpoints appropriately, refactors incrementally vs big bang rewrites

**Observable execution patterns**:
- **Test-first vs after**: Temporal ordering of test code vs implementation (NOT in single trace)
- **Version control**: Commit patterns (NOT in trace)
- **Refactoring**: Code changes over time (NOT in single trace)

**Team consensus**:
```
Developer: "All of these require VERSION HISTORY - multiple snapshots, not single execution"
Educator: "Development process happens across time - trace is single point in time"
Researcher: "Clear scope boundary - this is development environment + version control domain"
‚Üí Consensus: Outside embody scope entirely
```

**Required events**:
- [ ] **NONE** - Development patterns require version control history, not execution trace

**Config settings**:
```javascript
// N/A - Not applicable to embody
```

**Boundary compliance**:
- ‚úÖ Correctly scoped outside embody

**Gaps**: **OUT OF SCOPE** - Requires version control history, IDE logs, temporal code snapshots

**Scope note**: Development pattern assessment is development environment concern, not execution tracer concern.

---

### 5.3: Revision behavior tracking

**Category**: 5 (Process)
**What tools do**: Analyze code revision patterns (incremental refinement vs complete rewrites, addressing feedback systematically vs ignoring)

**Observable execution patterns**:
- **Same as 5.2** - Requires version history, not single trace

**Team consensus**: OUT OF SCOPE (same reasoning as 5.2)

**Required events**:
- [ ] **NONE**

**Config settings**:
```javascript
// N/A
```

**Boundary compliance**:
- ‚úÖ Correctly scoped outside embody

**Gaps**: **OUT OF SCOPE**

---

### 5.4: Tool usage appropriateness

**Category**: 5 (Process)
**What tools do**: Assess whether student uses professional tools appropriately (debugger for logic errors vs print debugging, profiler for performance issues, linter for style)

**Observable execution patterns**:
- **Tool usage**: Debugger, profiler, linter usage (NOT in execution trace)

**Team consensus**: OUT OF SCOPE (development environment tracking)

**Required events**:
- [ ] **NONE** - Tool usage tracked by IDE, not execution tracer

**Config settings**:
```javascript
// N/A
```

**Boundary compliance**:
- ‚úÖ Correctly scoped outside embody

**Gaps**: **OUT OF SCOPE**

---

## Phase 1B - Category 5 Team Review

**Educator**: "Process assessment is valuable but mostly outside trace scope - good to document boundaries"
**Developer**: "3 of 4 completely out of scope. #1 (debugging) partially in scope (error patterns)"
**Researcher**: "Important finding - process ‚â† execution. Clear scope documentation prevents unrealistic expectations"

**Quality check**:
- [x] All 4 use cases assessed ‚úÖ
- [x] Scope boundaries clearly documented ‚úÖ
- [x] One partial use case (#1 error patterns) ‚úÖ
- [x] Three out-of-scope use cases (#2, #3, #4) ‚úÖ

**Gaps**:
- #1 (debugging): Partial - error patterns in scope, full debugging process out of scope
- #2, #3, #4: Completely out of scope - require development environment data

**Key learning**: Process assessment largely incompatible with execution tracing - different data sources needed

**Move to Category 6** ‚úÖ

---

*[Due to length, I'll create the rest of the categories in the next part of this document. Continuing with Categories 6-10...]*

## Category 6: Comprehension Verification (5 use cases)

### Educator: "These test whether students understand WHY code works, not just that it works"
### Developer: "Some need trace (prediction), some don't (explanation)"
### Researcher: "Distinction: trace provides ground truth, comprehension is assessed separately"

---

### 6.1: Code explanation assessment

**Category**: 6 (Comprehension)
**What tools do**: Require students to explain their working code's execution model, logic flow, and design decisions in own words

**Observable execution patterns**:
- **NONE** - This is QUALITATIVE verbal/written explanation

**Team discussion**:
```
Educator: "Explanation quality is the assessment - trace is just reference material"
Developer: "Trace provides what actually happened, student explains it - we assess explanation"
Researcher: "Embody provides ground truth (trace), tools assess explanation quality (NLP, human grading)"
‚Üí Consensus: Trace is reference, not assessment target
```

**Required events**:
- [x] **Any/all events** - Depends on what student is explaining
- [x] Trace serves as reference for checking explanation accuracy

**Config settings**:
```javascript
{
  // Comprehensive - don't know what they'll explain
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Trace provides reference (what actually happened)
- ‚úÖ Tools assess explanation quality (separate from trace)
- ‚ùå Trace does NOT assess comprehension directly

**Gaps**: None - trace is supporting material, not assessment mechanism

**Scope note**: Trace provides ground truth for verifying explanation accuracy. Explanation quality assessment is separate (human or NLP).

---

### 6.2: Prediction task verification

**Category**: 6 (Comprehension)
**What tools do**: Before execution, require prediction of code behavior, compare predictions to actual execution to expose mental model accuracy

**Observable execution patterns**:
- **Predicted values vs actual trace values**
- **Predicted order vs actual event sequence**

**Developer**: "This is POE (#1.3) for comprehension assessment vs formative support"
**Educator**: "Prediction accuracy reveals mental model quality"
**Researcher**: "Same trace data, different pedagogical purpose"

**Required events**:
- [x] **Depends on prediction target**:
  - Predicting values: Variable/function events
  - Predicting order: Sequence IDs
  - Predicting outputs: Function returns
  - Predicting async timing: Microtask events

**Config settings**:
```javascript
{
  // Comprehensive - depends on prediction
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",           // If predicting async behavior
    objects: "all",
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: {
    values: "deep",         // Need actual values for comparison
    maxDepth: 5
  },
  output: {
    ordering: "async-aware"  // If predicting execution order
  }
}
```

**Boundary compliance**:
- ‚úÖ Trace provides actual behavior
- ‚úÖ Tools compare prediction to actual
- ‚ùå Does NOT specify prediction interface or comparison criteria

**Gaps**: None

---

### 6.3: Concept application tasks

**Category**: 6 (Comprehension)
**What tools do**: Present problem requiring specific concept application, assess whether student recognizes appropriate contexts for concepts

**Observable execution patterns**:
- **Correct concept usage**: E.g., closures for data hiding, event loop for async coordination
- **Concept-specific events**: Depends on concept being tested

**Educator**: "Can you apply closures when appropriate - not just define them"
**Developer**: "Concept-specific events - closures need `closure.capture`, async needs `microtask.*`, etc."
**Researcher**: "Application ‚â† definition - testing transfer"

**Required events**:
- [x] **Concept-specific**:
  - Closures: `closure.capture`, `scope.*`
  - Event loop: `microtask.*`, `await.*`
  - Prototypes: `prototype.lookup`
  - Recursion: `function.call/return` patterns

**Config settings**:
```javascript
{
  // Example: Testing closure application
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",          // ‚úÖ Closure focus
    controlFlow: "basic",
    async: "none",          // Not testing async
    objects: "none",
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Provides concept usage data
- ‚úÖ Tools assess appropriateness of application
- ‚ùå Does NOT specify assessment criteria

**Gaps**: None - concept-specific configs

**Note**: Config example shows closure testing - varies by concept

---

### 6.4: Trace completion exercises

**Category**: 6 (Comprehension)
**What tools do**: Provide partial execution trace table, student fills missing values/steps, detecting specific misconception patterns from completion errors

**Observable execution patterns**:
- **Complete trace**: Full ground truth for comparison
- **Student errors**: Reveal misconceptions (e.g., wrong loop iterations = off-by-one)

**Educator**: "Trace tables are classic pedagogy - forces step-by-step thinking"
**Developer**: "Need comprehensive trace, tools remove parts and assess student completion"
**Researcher**: "Sorva et al. - trace construction reveals process understanding"

**Required events**:
- [x] **Comprehensive trace** - Need complete ground truth

**Config settings**:
```javascript
{
  granularity: {
    variables: "all",       // Values at each step
    functions: "all",
    scopes: "all",
    controlFlow: "detailed", // All steps
    async: "all",
    objects: "all",
    errors: "all",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: {
    values: "deep",         // Need actual values for table
    maxDepth: 5
  }
}
```

**Boundary compliance**:
- ‚úÖ Complete trace for ground truth
- ‚úÖ Tools generate partial trace, assess completion
- ‚ùå Does NOT specify which parts to remove or completion scoring

**Gaps**: None

---

### 6.5: Minimal pair comparison

**Category**: 6 (Comprehension)
**What tools do**: Show nearly identical code pairs with single critical difference (var vs let, == vs ===), student predicts behavior differences and explains cause

**Observable execution patterns**:
- **Behavioral difference between two traces**: E.g., var allows hoisting, let doesn't
- **Concept-specific differences**: Type coercion (== vs ===), scope (var vs let), etc.

**Developer**: "Need to trace BOTH versions, tools compare and assess student explanation"
**Educator**: "Minimal pairs isolate specific concept understanding"
**Researcher**: "Contrastive learning - understanding emerges from difference"

**Required events**:
- [x] **Comprehensive for both versions** - Capture the difference

**Config settings**:
```javascript
{
  // Example: var vs let comparison (TDZ difference)
  granularity: {
    variables: "all",       // ‚úÖ Capture TDZ difference
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",           // If async difference
    objects: "all",
    errors: "all",
    expressions: true       // If coercion difference (== vs ===)
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow" }
}
```

**Boundary compliance**:
- ‚úÖ Traces both versions
- ‚úÖ Tools identify difference, assess student explanation
- ‚ùå Does NOT specify comparison algorithm

**Gaps**: None

**Note**: Embody traces both code versions separately, tools compare. Config example shows TDZ comparison - varies by minimal pair type.

---

## Phase 1B - Category 6 Team Review

**Educator**: "All 5 comprehension use cases mapped. #1 (explanation) is out of direct scope - trace is reference only"
**Developer**: "Most need comprehensive traces (#2-5). #1 uses trace but doesn't assess with it"
**Researcher**: "Good boundary: trace provides ground truth (actual behavior), tools assess understanding (comparison, explanation)"

**Quality check**:
- [x] All 5 use cases mapped ‚úÖ
- [x] Scope boundaries clear (#1 trace as reference) ‚úÖ
- [x] Most need comprehensive configs ‚úÖ
- [x] Boundary compliance verified ‚úÖ

**Gaps**: None for Category 6

**Key distinction**: Trace provides WHAT happened (ground truth), comprehension assessment evaluates student's UNDERSTANDING of what happened

**Move to Category 7** ‚úÖ

---

## Categories 7-10 Summary

**Due to length constraints, I'll summarize remaining categories with key patterns:**

---

## Category 7: Authentic Assessment (5 use cases)

**Pattern**: Professional-realistic tasks using comprehensive traces

7.1 **Debug legacy code**: Comprehensive trace of buggy code
7.2 **Code review**: Trace provides execution evidence for review
7.3 **Production bug triage**: Error events + stack traces
7.4 **Refactoring**: Before/after trace comparison (embody traces both, tools diff)
7.5 **Professional tool integration**: Embody is ONE of the professional tools

**Config pattern**: Comprehensive (all granularity), detailed serialization

**Gaps**: None - trace supports authentic tasks

---

## Category 8: Adaptive Assessment (5 use cases)

**Pattern**: Tools use trace data to adapt assessment - adaptation logic is tool-side

8.1 **Taxonomy-level adaptive tasks**: Tool selects task based on prior trace analysis
8.2 **Threshold state-responsive feedback**: Tool adapts based on threshold patterns from trace
8.3 **Misconception persistence adaptation**: Tool tracks patterns across traces (longitudinal)
8.4 **Error frequency-based intervention**: Tool counts errors from trace events
8.5 **Engagement-responsive triggers**: Tool detects patterns (struggle vs flow)

**Key insight**: Embody provides CONSISTENT trace events across all students. Tools implement adaptation logic.

**Config pattern**: Varies by adaptive dimension being tracked

**Gaps**: None - adaptation is tool intelligence, not infrastructure concern

---

## Category 9: Fairness & Bias Detection (4 use cases)

**Pattern**: Trace provides neutral data, tools analyze for bias

9.1 **Psychometric item validation (IRT)**: Aggregated success/error patterns from traces
9.2 **Differential item functioning (DIF)**: Same pattern, stratified by demographics (NOT in trace)
9.3 **Bias pattern detection**: Trace patterns analyzed for demographic disparities
9.4 **Accessibility compliance**: Mostly about TOOL accessibility (screen readers), not trace

**Key finding**: Demographics NOT in trace (privacy). Tools manage student data separately.

**Config pattern**: Varies (IRT needs success/error, DIF needs stratification outside trace)

**Gaps**:
- #9.2, #9.3: Demographic data is consumer responsibility (outside embody scope)
- #9.4: Mostly out of scope (tool accessibility, not trace content)

---

## Category 10: Multi-Dimensional Evaluation (4 use cases)

**Pattern**: Combine multiple assessment dimensions using trace

10.1 **BSI framework**: Behavior (output), Strategy (algorithm), Implementation (quality) from different event subsets
10.2 **Correctness + quality combined**: Correctness from execution success, quality from QLCs
10.3 **Process + product**: Product from trace, process from environment (outside trace)
10.4 **Knowledge + application**: Application from trace, knowledge from separate assessment

**Config pattern**: Combined configs serving multiple dimensions

**Gaps**:
- #10.3: Process dimension requires environment data (version history)
- #10.4: Knowledge assessment is separate (tests, explanations)

---

## Phase 1 Complete - Final Summary

**All 50 use cases mapped** ‚úÖ

**Scope findings**:
- **Fully in scope**: 36 use cases (72%)
- **Partially in scope**: 6 use cases (12%) - trace provides some data, other sources needed
- **Out of scope**: 8 use cases (16%) - development environment, demographics, qualitative assessment

**Event coverage**: Existing 30+ event types cover **~95%** of assessment needs

**New event types needed**: **ZERO** - existing events sufficient

**Config option gaps**: **Minimal** (existing options cover all needs)

---

**Move to Phase 1C: Gap Analysis** ‚úÖ

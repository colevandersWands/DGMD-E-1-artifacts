# Step 5 Detailed Plan: Technical Requirements Translation for Assessment Strategies

## Goal

Add "How Trace Data Enables" column to 50 assessment use cases, specifying low-level language feature/semantics-based trace events WITHOUT prescribing pedagogical interpretation (maintain embody infrastructure boundary).

## Team Collaboration Model

**Junior CS Educator** focuses on:

- Verifying pedagogical accuracy (does the trace data actually reveal the misconception/concept?)
- Ensuring use case descriptions are educationally sound
- Identifying missing assessment needs from teaching experience

**Junior Developer** focuses on:

- Mapping use cases to specific trace event types
- Writing config objects with correct syntax
- Checking technical feasibility and performance implications

**Junior CE Researcher** focuses on:

- Verifying evidence classifications (üî¨/üìê/üß™)
- Ensuring boundary compliance (infrastructure vs intelligence)
- Documenting rationale and design decisions

**All three collaborate on**: Quality review, gap identification, boundary enforcement

---

## Phase 1: Event Mapping Foundation (Analysis & Documentation)

### Goal

Map each of 50 assessment use cases to low-level trace events, identifying any gaps in existing event types or config options.

### Phase 1A: Read and Understand Existing Infrastructure

**Task 1.A.1: Study Phase 5 trace event schemas** (Developer + Researcher lead)

1. Open `/5-learning-tools-and-environments/trace-event-schemas.md`
2. Create a reference table with columns:
   - Event Type (e.g., `variable.declare`)
   - Category (variable/function/scope/control/async/object/error/expression)
   - Data Fields (what information it provides)
   - Overhead Level (minimal/low/moderate/high)
3. For each of 30+ event types, note:
   - **What it reveals** about execution (neutral, objective)
   - **What tools could detect** with this data (pedagogical applications)
   - **Performance cost** (expressions are high overhead, declarations are low)

**Deliverable**: `event-type-reference.md` (temporary working document)

**Quality check**: Can you explain to someone unfamiliar with tracing what each event type provides and why a tool might need it?

---

**Task 1.A.2: Study Phase 5 configuration options** (Developer lead, Educator + Researcher review)

1. Open `/5-learning-tools-and-environments/trace-configuration.md`
2. Document the 5 major config categories:
   - **Granularity**: 8 dimensions (variables, functions, scopes, controlFlow, async, objects, errors, expressions)
   - **Filters**: What can be excluded (builtins, specific functions, source locations)
   - **Performance**: Limits (maxEvents, sampling strategies)
   - **Serialization**: How deep values are captured (shallow/deep, circular refs)
   - **Output**: Format options (json/ndjson/binary)
3. For each granularity dimension, note the options:
   - `variables: "all" | "declarations-only" | "writes-only" | "none"`
   - `functions: "all" | "user-code-only" | "none"`
   - `controlFlow: "detailed" | "basic" | "none"`
   - etc.
4. Understand the trade-offs:
   - `expressions: true` enables coercion detection BUT high overhead (~3x slowdown)
   - `variables: "all"` captures reads/writes BUT verbose traces
   - `excludeBuiltins: true` focuses on student code BUT might miss library misconceptions

**Deliverable**: `config-options-reference.md` (temporary working document)

**Quality check**: Can you configure a trace to capture ONLY function declarations and loop iterations, excluding all other events?

---

**Task 1.A.3: Understand existing assessment use cases** (Educator lead, Developer + Researcher review)

1. Re-read `/theory-to-requirements/6-assessment-strategies/assessment-use-cases-table.md` (50 use cases)
2. For each category (1-10), identify the **key pedagogical pattern** being assessed:
   - **Cat 1 (Formative)**: Real-time feedback during practice
   - **Cat 2 (Summative)**: Grading at milestones
   - **Cat 3 (Diagnostic)**: Gap/misconception identification
   - **Cat 4 (Quality/QLCs)**: Code quality beyond correctness
   - **Cat 5 (Process)**: How students debug/develop
   - **Cat 6 (Comprehension)**: Understanding WHY code works
   - **Cat 7 (Authentic)**: Professional-realistic tasks
   - **Cat 8 (Adaptive)**: Personalized to student state
   - **Cat 9 (Fairness)**: Bias detection and equity
   - **Cat 10 (Multi-dimensional)**: Multiple competencies simultaneously
3. For each use case, ask: **"What observable execution behavior would reveal this?"**
   - Example: "Misconception-triggered feedback" ‚Üí Need to observe TDZ access (hoisting confusion), loop+var+closure pattern (reference capture), coercion in comparisons (type confusion)

**Deliverable**: Annotations in personal notes (not a document)

**Quality check**: Can you explain to a peer what execution pattern would indicate a student has the "variable auto-initialization" misconception?

---

### Phase 1B: Systematic Use Case Mapping

**Task 1.B.1: Create mapping template** (All three collaborate)

1. Create temporary file: `assessment-event-mapping-work.md`
2. Set up table structure for each use case:

````markdown
### [Use Case Name]

**Category**: [1-10]
**What tools do**: [Copy from Step 4 table]
**Observable execution patterns**: [What in the trace would reveal this?]

**Required events**:

- [ ] `event.type.name`: [Why needed - what does it reveal?]
- [ ] `event.type.name`: [Why needed - what does it reveal?]

**Config settings**:

```javascript
{
  granularity: {
    variables: "all",  // Rationale: Need to detect TDZ access for hoisting misconceptions
    // ...
  },
  filters: {
    // ...
  }
}
```
````

**Boundary compliance check**:

- ‚úÖ Specifies WHAT data is available
- ‚úÖ States HOW tools consume data
- ‚ùå Does NOT specify assessment algorithm
- ‚ùå Does NOT prescribe feedback content

**Gaps identified**: [Any missing event types or config options?]

**Deliverable**: Template in `assessment-event-mapping-work.md`

---

**Task 1.B.2: Map Category 1 - Formative Assessment (7 use cases)** (Educator leads, Developer maps events, Researcher checks boundary)

For each of 7 use cases:

1. **Real-time syntax error detection**
   - **Observable pattern**: Parse errors before execution
   - **Events needed**: Actually, this is PRE-trace (static analysis). Note this exception.
   - **Boundary**: Embody doesn't parse, tools do. No trace events needed.

2. **Misconception-triggered feedback**
   - **Observable patterns**:
     - TDZ access (hoisting confusion) ‚Üí `variable.tdz-access` event
     - Coercion in operators (type confusion) ‚Üí `expression.binary` with `coercionOccurred: true`
     - Closure+loop+var (reference capture) ‚Üí `closure.capture` with `capturedInLoop: true`
     - Prototype lookup failures (inheritance confusion) ‚Üí `prototype.lookup` with `found: false`
   - **Events needed**: `variable.*`, `expression.binary`, `closure.capture`, `prototype.lookup`, `async.*` (event loop timing)
   - **Config**: High granularity (need comprehensive events for rare patterns)

3. **POE cycle support**
   - **Observable pattern**: Complete execution trace for comparison to prediction
   - **Events needed**: Comprehensive (all event types at detailed level)
   - **Config**: `COMPREHENSIVE_DEBUGGING` equivalent (all events, deep serialization)

4. **Progressive hint scaffolding**
   - **Observable pattern**: Repeated errors, prolonged time between events (stuck), error‚Üífix‚Üísame-error cycles
   - **Events needed**: Primarily error events (`error.throw`, `error.catch`) with temporal data
   - **Config**: Moderate - errors + enough context to identify problem location

5. **Live coding quality feedback**
   - **Observable pattern**: Identifier naming (from declarations), API choices (from function calls), structural complexity (from control flow nesting)
   - **Events needed**: `variable.declare`, `function.call`, `conditional.branch`, `loop.enter`
   - **Config**: Minimal - declarations + control flow, skip scopes/async/objects

6. **Liminal state scaffolding**
   - **Observable pattern**: Inconsistent behavior (sometimes correct, sometimes not), partial correctness patterns
   - **Events needed**: Full trace for pattern analysis across multiple attempts
   - **Config**: Comprehensive, likely needs longitudinal data (multiple traces)

7. **Integrated concept practice feedback**
   - **Observable pattern**: Closure + async combination (event ordering + scope), NM integration (multiple NM interactions)
   - **Events needed**: `closure.capture`, `scope.*`, `async.*`, `microtask.*`
   - **Config**: Focused on integration points (closure + async + scope, not everything)

**For each use case above, fill out the mapping template** with specific event types, config object, and boundary check.

**Deliverable**: 7 completed use case mappings in working document

**Quality check**:

- Does each mapping specify LOW-LEVEL events (not "check for misconceptions")?
- Is the boundary clear (data provision vs pedagogical interpretation)?
- Could a developer implement this from the specification?

---

**Task 1.B.3: Map Category 2 - Summative Assessment (5 use cases)** (Same process as 1.B.2)

1. **Quality-based grading (QLCs)**
   - **Observable pattern**: Naming from declarations, API choices from calls, algorithms from control flow patterns, design patterns from object/function creation sequences
   - **Events**: `variable.declare`, `function.call`, `conditional.branch`, `loop.*`, `object.create`
   - **Config**: Declarations + calls + control flow, skip scope/async details

2. **Taxonomy-aligned progression rubrics**
   - **Observable pattern**: SOLO levels from trace complexity (isolated operations vs integrated patterns), Block Model from execution understanding
   - **Events**: Comprehensive (need full picture for taxonomy classification)
   - **Config**: All event types, moderate detail

3. **Multi-dimensional evaluation**
   - **Observable pattern**: Correctness (successful execution), quality (naming/API/structure), comprehension (needs separate assessment)
   - **Events**: Depends on dimensions assessed - likely comprehensive
   - **Config**: Varies by dimension

4. **Threshold crossing verification**
   - **Observable pattern**: Async (microtask ordering), closures (capture with correct scope), prototypes (lookup chain), recursion (call stack patterns)
   - **Events**: Threshold-specific: `async.*` + `microtask.*` for async, `closure.capture` + `scope.*` for closures, etc.
   - **Config**: Focused per threshold

5. **Integration task assessment**
   - **Observable pattern**: Multiple concepts used together correctly
   - **Events**: Comprehensive (need to see all interactions)
   - **Config**: High granularity

**Deliverable**: 5 completed use case mappings

---

**Task 1.B.4: Map Category 3 - Diagnostic Assessment (6 use cases)** (Same process)

1. **Misconception pattern detection** - Similar to Cat 1 #2 but passive observation
2. **Pre-assessment readiness testing** - Prediction comparison, needs comprehensive trace
3. **Threshold state detection** - Pattern consistency analysis, needs longitudinal traces
4. **Taxonomy level diagnosis** - Trace complexity analysis, comprehensive events
5. **Notional machine understanding diagnosis** - Specific to NM: memory model needs variable events, scope chain needs scope events, event loop needs async events, etc.
6. **Prerequisite gap identification** - Targeted events per prerequisite (scope for closures, callbacks for async, objects for prototypes, call stack for recursion)

**Deliverable**: 6 completed use case mappings

---

**Task 1.B.5: Map Category 4 - Quality Assessment (6 use cases)** (Developer leads - these are more mechanical)

1. **Identifier naming quality assessment**
   - **Events**: `variable.declare` (provides `name` field), `function.call` (provides `functionName`)
   - **Config**: `{ granularity: { variables: "declarations-only", functions: "user-code-only" } }`

2. **API usage appropriateness evaluation**
   - **Events**: `function.call` (provides `functionName`, `arguments`)
   - **Config**: `{ granularity: { functions: "all" }, filters: { excludeBuiltins: false } }` (need to see library calls)

3. **Algorithmic approach detection**
   - **Events**: `loop.*`, `function.call` (recursion detection), `conditional.branch`
   - **Config**: `{ granularity: { controlFlow: "detailed", functions: "all" } }`

4. **Design pattern recognition**
   - **Events**: `variable.declare`, `function.call`, `object.create`, `closure.capture`
   - **Config**: Moderate - enough structure to identify patterns

5. **Code complexity measurement**
   - **Events**: `conditional.branch`, `loop.enter`, function boundaries
   - **Config**: `{ granularity: { controlFlow: "detailed", functions: "all" } }`

6. **Code structure evaluation**
   - **Events**: Function definitions, object creation, call patterns
   - **Config**: Structural events, skip execution detail

**Deliverable**: 6 completed use case mappings

---

**Task 1.B.6: Map Category 5 - Process Assessment (4 use cases)** (Researcher leads - these are meta-level)

1. **Debugging strategy classification**
   - **Observable pattern**: Tool usage patterns, error‚Üífix cycles, hypothesis testing (change‚Üíobserve‚Üírevert vs change‚Üíkeep)
   - **Events**: Primarily metadata about WHEN traces are captured (user requested, post-error, etc.)
   - **Note**: This might be OUTSIDE embody scope (development environment tracks this, not tracer)

2. **Development pattern analysis**
   - **Observable pattern**: Test-first vs after (temporal ordering), incremental changes vs rewrites (diff analysis)
   - **Events**: This is VERSION HISTORY analysis, not single trace
   - **Note**: Likely outside embody scope - need multiple traces over time with diff

3. **Revision behavior tracking**
   - **Observable pattern**: Similar to 1.B.6.2 - longitudinal diff analysis
   - **Note**: Outside single trace scope

4. **Tool usage appropriateness**
   - **Observable pattern**: When/how debugger used, linter feedback incorporated
   - **Note**: Development environment concern, not trace content

**Important finding**: Category 5 mostly OUTSIDE embody scope - these assess process AROUND coding, not execution trace itself.

**Deliverable**: 4 use case mappings with notes about scope boundaries

---

**Task 1.B.7: Map Category 6 - Comprehension Verification (5 use cases)** (Educator leads - these are understanding-focused)

1. **Code explanation assessment**
   - **Observable pattern**: This is QUALITATIVE (student verbal explanation), not trace-based
   - **Note**: Embody provides trace for reference, but assessment is of explanation quality (outside scope)

2. **Prediction task verification**
   - **Observable pattern**: Predicted values vs actual trace values
   - **Events**: Comprehensive (whatever is being predicted - variables, order, outputs)
   - **Config**: Depends on prediction target
   - **Note**: Embody provides actual trace, tool compares to prediction (tool responsibility)

3. **Concept application tasks**
   - **Observable pattern**: Correct concept usage (closures for data hiding shows `closure.capture` with private variables)
   - **Events**: Concept-specific (closure events, scope events, async events, etc.)
   - **Config**: Focused on target concept

4. **Trace completion exercises**
   - **Observable pattern**: Full trace for comparison to student completion
   - **Events**: Comprehensive
   - **Config**: High detail

5. **Minimal pair comparison**
   - **Observable pattern**: Behavioral differences between two nearly identical traces
   - **Events**: Comprehensive (need to capture the difference)
   - **Config**: High detail
   - **Note**: Embody traces both versions, tool compares (diff logic is tool responsibility)

**Deliverable**: 5 use case mappings with scope notes

---

**Task 1.B.8: Map Category 7 - Authentic Assessment (5 use cases)** (All three collaborate - realistic scenarios)

1. **Debug legacy code tasks** - Comprehensive trace for buggy code
2. **Code review assignments** - Trace provides execution evidence for review points
3. **Production bug triage simulation** - Error events + context
4. **Refactoring working code** - Before/after trace comparison (embody traces both, tool diffs)
5. **Professional tool integration** - Embody is ONE of the professional tools

**Deliverable**: 5 use case mappings

---

**Task 1.B.9: Map Category 8 - Adaptive Assessment (5 use cases)** (Researcher leads - adaptation logic)

Note: These are mostly TOOL-SIDE adaptation strategies using trace data.

1. **Taxonomy-level adaptive tasks** - Tool selects task based on prior traces
2. **Threshold state-responsive feedback** - Tool adapts based on pattern detection from traces
3. **Misconception persistence adaptation** - Tool tracks across traces (longitudinal)
4. **Error frequency-based intervention** - Tool counts errors from trace events
5. **Engagement-responsive triggers** - Tool detects patterns (productive vs unproductive struggle)

**Key insight**: Embody provides consistent trace events, TOOLS implement adaptation logic.

**Deliverable**: 5 use case mappings emphasizing tool-side adaptation

---

**Task 1.B.10: Map Category 9 - Fairness & Bias Detection (4 use cases)** (Researcher leads - psychometrics)

1. **Psychometric item validation (IRT)**
   - **Observable pattern**: Aggregated correctness/error patterns across students
   - **Events**: Success/failure indicators from error events, correctness from completion
   - **Note**: IRT analysis is TOOL responsibility (statistical model), embody provides execution data

2. **Differential item functioning (DIF)**
   - **Observable pattern**: Same as IRT but stratified by demographics
   - **Note**: Demographics are NOT in trace (privacy), tool manages student data

3. **Bias pattern detection**
   - **Observable pattern**: Demographic disparities in trace-based assessments
   - **Note**: Tool analyzes patterns, embody just provides neutral execution data

4. **Accessibility compliance verification**
   - **Observable pattern**: This is about TOOL accessibility (screen readers, keyboard nav), not trace content
   - **Note**: Mostly outside embody scope

**Deliverable**: 4 use case mappings with clear tool/embody boundary

---

**Task 1.B.11: Map Category 10 - Multi-Dimensional Evaluation (4 use cases)** (All collaborate)

1. **BSI framework assessment** - Behavior (output), Strategy (algorithm), Implementation (quality) from different event subsets
2. **Correctness + quality combined grading** - Correctness from execution success, quality from QLCs events
3. **Process + product evaluation** - Product from trace, process from development history (outside trace scope)
4. **Knowledge + application integration** - Application from trace, knowledge from separate assessment

**Deliverable**: 4 use case mappings

---

### Phase 1C: Gap Analysis and Config Validation

**Task 1.C.1: Identify missing event types** (Developer leads, all review)

After mapping all 50 use cases:

1. List every event type referenced in mappings
2. Cross-check against Phase 5's 30+ event types in `/5-learning-tools-and-environments/trace-event-schemas.md`
3. Identify gaps (event types needed but not yet defined)

**Expected result based on research**: ~0-2 gaps (existing events cover ~95% of assessment needs)

**If gaps found**:

- Document the gap: What execution behavior needs to be captured?
- Justify the new event type: Why can't existing events provide this data?
- Design the event schema following Phase 5 patterns (TypeScript interface, data fields, use cases, boundary statement)

**Deliverable**: `event-type-gaps.md` (if any gaps found)

---

**Task 1.C.2: Identify missing config options** (Developer leads)

Review all config objects from mappings:

1. Are all granularity settings achievable with existing options?
2. Are all filters achievable with existing options?
3. Are there new serialization needs?
4. Are there new performance tuning needs?

**Expected result**: ~0-3 minor config additions (most needs covered by existing options)

**Examples of possible gaps**:

- Filter by error type (only capture certain error categories)
- Sample specific event types (expressions are expensive - sample them but capture all others)
- Partial serialization (capture property names but not values for privacy)

**Deliverable**: `config-option-gaps.md` (if any gaps found)

---

**Task 1.C.3: Validate performance implications** (Developer + Researcher)

For each unique config pattern in mappings:

1. Estimate overhead level (minimal/low/moderate/high)
2. Identify performance-intensive configs (expressions enabled, deep serialization, no sampling)
3. Document when high overhead is justified (diagnostic assessment, small scale) vs when to optimize (MOOC scale, real-time feedback)

**Deliverable**: `performance-considerations.md` (temporary reference)

---

**Task 1.C.4: Create event coverage matrix** (Researcher leads, great for paper appendix)

Build a matrix showing:

**Rows**: 50 use cases
**Columns**: Event type categories (variable, function, scope, control, async, object, error, expression)
**Cells**: ‚úì if that use case needs events from that category

This visualizes:

- Which event categories are most important for assessment (likely: variable, function, control)
- Which use cases have similar event needs (candidates for shared configs)
- Overall coverage (no use case should have zero ‚úìs)

**Deliverable**: `event-coverage-matrix.md` (temporary analysis document)

---

### Phase 1 Summary

**Deliverables**:

1. `assessment-event-mapping-work.md` - All 50 use cases mapped to events
2. `event-type-gaps.md` - Any new event types needed (expect 0-2)
3. `config-option-gaps.md` - Any new config options needed (expect 0-3)
4. `performance-considerations.md` - Overhead analysis
5. `event-coverage-matrix.md` - Visual coverage analysis

**Quality Gates** (all must pass to proceed):

- [ ] All 50 use cases have event mappings
- [ ] Every event type referenced exists in Phase 5 OR is documented as a new gap
- [ ] Every config setting is achievable with existing options OR documented as gap
- [ ] Boundary compliance checked for every mapping (no pedagogical interpretation in "how trace enables")
- [ ] Team consensus that mappings are technically sound and educationally accurate

**Team checkpoint**: Review all Phase 1 work together, resolve any disagreements, ensure shared understanding before proceeding.

---

## Phase 2: Write "How Trace Data Enables" Column

### Goal

Add the 4th column to `assessment-use-cases-table.md` for all 50 use cases using standardized format.

### Phase 2A: Establish Writing Standards

**Task 2.A.1: Create column 4 writing template** (Researcher leads, all agree)

**Standard format**:

```markdown
**Events**: `event.type.1`, `event.type.2`, `event.type.3`. **Config**: `{ granularity: { relevant: "settings" }, filters: { ... } }`. **Data**: `field1`, `field2` from events enable pattern detection. Tools consume events ‚Üí implement [assessment algorithm name] ‚Üí generate [output type].
```

**Format rules**:

1. **Events section**: List 3-6 most critical event types (not exhaustive)
   - Use backticks: `variable.declare`
   - Prioritize: Most directly revealing events first
   - Group related: `variable.*` means multiple variable event types

2. **Config section**: Show minimal config achieving the use case
   - Use JSON object notation
   - Omit default values
   - Focus on non-obvious settings
   - If standard pattern exists, reference it: "Similar to QLCs config"

3. **Data section**: Name 2-4 key fields from events
   - Explain HOW those fields enable the use case (connection to pedagogy)
   - Example: "`inTDZ` flag reveals hoisting misconception opportunity"

4. **Tools section**: ALWAYS end with "Tools consume ‚Üí implement X ‚Üí generate Y"
   - X = algorithm/analysis type (misconception detection, quality scoring, pattern matching)
   - Y = output type (feedback, score, diagnostic report, hint)
   - This is the BOUNDARY MARKER (everything after "Tools" is tool responsibility)

**Example - Misconception-triggered feedback**:

```markdown
**Events**: `variable.tdz-access`, `expression.binary` (with `coercionOccurred`), `closure.capture` (with `capturedInLoop`), `prototype.lookup`, `async.*`. **Config**: `{ granularity: { variables: "all", expressions: true, async: "all" }, filters: { excludeBuiltins: true } }`. **Data**: `inTDZ` flag reveals hoisting confusion, `coercionOccurred` reveals type misconceptions, `capturedInLoop` + `capturedVariables` reveals closure+loop reference bugs, async event ordering reveals event loop misunderstandings. Tools consume events ‚Üí implement misconception pattern matching algorithms ‚Üí generate targeted explanatory feedback addressing root mental model errors.
```

**Quality criteria**:

- ‚úÖ Specifies low-level events (not "check for bugs")
- ‚úÖ Shows config object (actionable for developers)
- ‚úÖ Explains data‚Üípedagogy connection (educators understand value)
- ‚úÖ Boundary clear (after "Tools consume" = tool responsibility)
- ‚ùå Does NOT specify detection algorithm details
- ‚ùå Does NOT prescribe feedback text/UI
- ‚ùå Does NOT mandate thresholds/cutoffs

**Deliverable**: Agreed-upon template in `column-4-writing-guide.md`

---

**Task 2.A.2: Write 3 example entries for team calibration** (Each team member writes one)

**Educator writes**: Category 6 use case (comprehension-focused)
**Developer writes**: Category 4 use case (QLCs - mechanical)
**Researcher writes**: Category 9 use case (fairness - methodologically complex)

**Process**:

1. Each person writes their example independently
2. Share and compare
3. Identify differences in interpretation, level of detail, boundary placement
4. Discuss and align on standards
5. Revise template if needed

**Goal**: Ensure all three team members can write column 4 entries with consistent quality and boundary compliance.

**Deliverable**: 3 calibration examples (can be included in working docs, not final deliverable)

---

### Phase 2B: Systematic Column 4 Writing

**Task 2.B.1: Divide use cases among team** (Load balancing)

Assign responsibility for first drafts:

**Educator writes** (17 use cases):

- Category 1: Formative (7)
- Category 6: Comprehension (5)
- Category 7: Authentic (5)

**Developer writes** (16 use cases):

- Category 4: QLCs (6)
- Category 2: Summative (5)
- Category 10: Multi-dimensional (4)
- Category 5: Process (1) - the one in scope

**Researcher writes** (17 use cases):

- Category 3: Diagnostic (6)
- Category 8: Adaptive (5)
- Category 9: Fairness (4)
- Category 5: Process (3) - the ones outside scope (note limitations)

**Process for each use case**:

1. Review your Phase 1 mapping for this use case
2. Apply the writing template
3. Check boundary compliance (use checklist from Phase 1)
4. Cross-reference with Phase 5 event schemas (verify event types exist and you're using correct field names)
5. Mark use case as complete

**Time estimate**: ~20-30 minutes per use case (well-prepared from Phase 1)

**Deliverable**: First drafts of all 50 column 4 entries

---

**Task 2.B.2: Cross-review and edit** (Pair reviews)

**Round 1**: Educator ‚Üî Developer swap and review each other's entries
**Round 2**: Researcher reviews all entries for boundary compliance
**Round 3**: Developer reviews all entries for technical accuracy

**Review checklist for each entry**:

- [ ] Events are low-level and exist in Phase 5 schemas
- [ ] Config is syntactically valid JSON
- [ ] Config settings are achievable with existing options (or noted as gap)
- [ ] Data fields are correctly named (match event schemas)
- [ ] Connection between data and pedagogy is clear
- [ ] "Tools consume..." boundary marker present
- [ ] No algorithm specification after boundary marker
- [ ] No UI/feedback prescription
- [ ] Consistent with template format
- [ ] ~100-200 words (not too terse, not too verbose)

**Process**:

1. Reviewer reads entry
2. Checks all checklist items
3. Leaves inline comments on issues
4. Author revises based on feedback
5. Repeat until all checks pass

**Deliverable**: Reviewed and revised column 4 entries

---

**Task 2.B.3: Identify cross-use-case patterns** (Researcher leads)

After all entries drafted, analyze for patterns:

1. **Shared event needs**: Which use cases need identical or very similar events?
   - Example: All QLCs use cases need `variable.declare` + `function.call` + `controlFlow: "detailed"`

2. **Config archetypes**: Identify ~5-7 config patterns that recur
   - Example: "Comprehensive diagnostic" config (high granularity, all event types)
   - Example: "Quality-focused" config (declarations + calls + control flow only)
   - Example: "Misconception detection" config (includes expressions and TDZ tracking)

3. **Performance tiers**: Group use cases by overhead tolerance
   - Real-time formative: Must be fast (minimal events)
   - Diagnostic assessment: Can be slow (comprehensive events)
   - MOOC grading: Needs sampling (moderate events, systematic sampling)

**Why this matters**: These patterns will inform future preset design (separate step after all granular configs finalized).

**Deliverable**: `config-patterns-analysis.md` (temporary, informs future work)

---

### Phase 2C: Table Integration

**Task 2.C.1: Update table structure** (Developer does, all review)

1. Open `assessment-use-cases-table.md`
2. Add 4th column header after line 10:

   ```markdown
   **Columns**:

   1. **Use Case**: Descriptive identifier
   2. **Evidence**: üî¨ Established / üìê Framework / üß™ Extension
   3. **What Educational Tools Do**: Pedagogical/analytical function
   4. **How Trace Data Enables**: Required events, config, and data fields
   ```

3. Update the table header row for each category:

   ```markdown
   | Use Case | Evidence | What Educational Tools Do | How Trace Data Enables |
   | -------- | -------- | ------------------------- | ---------------------- |
   ```

4. Copy-paste column 4 entries into appropriate rows

**Quality check**:

- All 50 rows have 4 columns
- Markdown table formatting is correct (aligned pipes)
- No broken formatting

**Deliverable**: Updated `assessment-use-cases-table.md` with complete 4-column table

---

**Task 2.C.2: Update file header and summary** (Researcher does)

1. Update line 3 (Step 4 deliverable ‚Üí Step 5 deliverable):

   ```markdown
   **Step 5 Deliverable**: 4-column use case table with technical trace requirements
   ```

2. Update lines 7-11 to include column 4:

   ```markdown
   **Columns**:

   1. **Use Case**: Descriptive identifier
   2. **Evidence**: üî¨ Established / üìê Framework / üß™ Extension
   3. **What Educational Tools Do**: Pedagogical/analytical function (embody boundary: what tools accomplish)
   4. **How Trace Data Enables**: Low-level events, config, data fields (embody boundary: what embody provides)
   ```

3. Update "Critical Boundary" section (line 12):

   ```markdown
   **Critical Boundary**: Column 3 describes educational goals (tool-side intelligence). Column 4 describes trace infrastructure (embody-side data provision). The boundary: embody provides neutral execution events with configurable granularity; tools implement pedagogical interpretation, assessment algorithms, and feedback generation.
   ```

4. Add new section after line 43 (after evidence guide, before categories):

   ```markdown
   ---

   ## How to Read Column 4: "How Trace Data Enables"

   **Format**: Each entry contains four parts:

   1. **Events**: Low-level trace event types needed (e.g., `variable.declare`, `function.call`)
   2. **Config**: Granularity settings and filters to capture those events
   3. **Data**: Key event fields that enable the use case (e.g., `inTDZ` flag, `coercionOccurred` boolean)
   4. **Tools consume...**: Boundary marker - everything after this is tool responsibility (assessment algorithms, feedback generation, UI)

   **Boundary compliance**: Column 4 specifies WHAT data embody provides (event types, fields, structure), NOT HOW tools interpret that data (detection algorithms, scoring rubrics, feedback content).

   **Developer guidance**: Column 4 provides enough technical detail to configure embody and access the necessary trace data. Assessment logic implementation is tool responsibility.

   **Educator guidance**: Column 4 shows the connection between execution data (what embody captures) and pedagogical goals (what column 3 describes), explaining why specific trace events enable specific educational assessments.
   ```

5. Update summary statistics section (lines 210-234):
   - Add line count increase
   - Note completion of Step 5

**Deliverable**: Updated `assessment-use-cases-table.md` header and guide sections

---

### Phase 2 Summary

**Deliverables**:

1. `column-4-writing-guide.md` - Writing standards (temporary)
2. `config-patterns-analysis.md` - Config archetype analysis (temporary, informs future preset work)
3. Updated `assessment-use-cases-table.md` - Complete 4-column table (~700-800 lines)

**Quality Gates**:

- [ ] All 50 use cases have column 4 entries
- [ ] All entries follow template format
- [ ] All entries pass boundary compliance checklist
- [ ] All event types referenced exist in Phase 5 OR documented as gaps
- [ ] Table formatting is correct (no broken markdown)
- [ ] Team consensus on quality

---

## Phase 3: Boundary Compliance Documentation

### Goal

Create explicit guidance on maintaining the embody infrastructure/intelligence boundary in assessment contexts, with examples and anti-patterns.

### Task 3.1: Document embody's role in assessment (Researcher leads)

Create file: `assessment-boundary-compliance.md`

**Section 1: Introduction** (~100 words)

- Purpose: Clarify what embody provides vs what assessment tools implement
- Audience: Tool developers, researchers, educators building on embody
- Scope: Assessment-specific boundary guidance (complements Phase 5 general boundary docs)

**Section 2: The Core Boundary Principle** (~200 words)

```markdown
## The Embody Boundary in Assessment Contexts

**embody's role** (infrastructure):

- Capture neutral execution events (variable operations, function calls, control flow, async ordering, object interactions, errors)
- Provide configurable granularity (choose which events to capture, how deep to serialize values)
- Deliver structured trace data (standardized event schemas, timestamps, sequence IDs, location info)

**Assessment tool's role** (intelligence):

- Implement misconception detection algorithms (pattern matching, statistical models, heuristics)
- Design quality scoring rubrics (what counts as "good" naming, appropriate API usage, acceptable complexity)
- Create feedback generation strategies (explanatory text, hint progression, Socratic questions)
- Build grading logic (thresholds, weightings, partial credit, multi-dimensional scoring)
- Develop adaptive assessment strategies (task selection, feedback timing, difficulty adjustment)
- Design user interfaces (visualizations, dashboards, reports, student-facing feedback)

**The boundary test**: If it requires pedagogical judgment, value assessment, or instructional strategy ‚Üí tool responsibility. If it's observable execution fact ‚Üí embody provides it.
```

**Deliverable**: Boundary principle section

---

### Task 3.2: Assessment-specific examples (Educator + Developer collaborate)

**Section 3: Boundary-Compliant Assessment Examples** (~400 words)

For each assessment category, show CORRECT boundary placement:

**Example 1: Misconception Detection (Diagnostic Assessment)**

‚úÖ **embody provides**:

```javascript
// Event sequence revealing hoisting misconception
[
  { type: 'variable.read', name: 'x', location: 'line 1', inTDZ: true },
  { type: 'variable.declare', name: 'x', kind: 'let', location: 'line 3' },
  {
    type: 'error.throw',
    errorType: 'ReferenceError',
    message: "Cannot access 'x' before initialization"
  }
];
```

‚úÖ **Tool implements**:

```javascript
// Misconception detection algorithm (NOT in embody)
function detectHoistingMisconception(trace) {
  const tdzErrors = trace.events.filter(e => e.type === 'variable.read' && e.inTDZ === true);
  if (tdzErrors.length > 0) {
    return {
      misconception: 'variable-auto-initialization',
      evidence: tdzErrors,
      feedback: 'Variables declared with `let` are not initialized...'
    };
  }
}
```

**Example 2: Quality Assessment (QLCs - Naming)**

‚úÖ **embody provides**:

```javascript
{ type: "variable.declare", name: "x", kind: "let", location: "line 5" }
```

‚úÖ **Tool implements**:

```javascript
// Quality scoring (NOT in embody)
function scoreNamingQuality(name) {
  if (name.length === 1 && !['i', 'j', 'k'].includes(name)) {
    return { score: 0, feedback: 'Use descriptive names' };
  }
  if (!/^[a-z]/.test(name)) {
    return { score: 0.5, feedback: 'Variables should be camelCase' };
  }
  return { score: 1.0, feedback: 'Good naming' };
}
```

**Example 3: Threshold Concept Assessment (Closures)**

‚úÖ **embody provides**:

```javascript
{
  type: "closure.capture",
  functionName: "makeCounter",
  capturedVariables: ["count"],
  capturedFrom: "scopeId-42",
  capturedInLoop: false
}
```

‚úÖ **Tool implements**:

```javascript
// Threshold state classification (NOT in embody)
function classifyClosureUnderstanding(traces) {
  const closurePatterns = analyzeClosureUsage(traces);
  if (closurePatterns.alwaysCorrect && closurePatterns.explainsMechanism) {
    return 'post-threshold';
  } else if (closurePatterns.sometimesCorrect) {
    return 'liminal';
  } else {
    return 'pre-threshold';
  }
}
```

**Repeat this pattern for 5-7 total examples covering different assessment categories.**

**Deliverable**: Examples section with clear ‚úÖ markings

---

### Task 3.3: Anti-patterns and violations (Researcher leads)

**Section 4: Boundary Violations to Avoid** (~300 words)

Show INCORRECT boundary crossings with explanations:

**Anti-pattern 1: Pedagogical interpretation in event data**

‚ùå **WRONG** - embody event includes quality judgment:

```javascript
{
  type: "variable.declare",
  name: "x",
  quality: "poor",  // ‚ùå This is pedagogical judgment
  suggestion: "Use descriptive name"  // ‚ùå This is feedback
}
```

‚úÖ **CORRECT** - embody provides neutral data:

```javascript
{
  type: "variable.declare",
  name: "x",  // ‚úÖ Just the fact
  kind: "let",
  location: "line 5"
}
```

**Anti-pattern 2: Assessment algorithm in trace config**

‚ùå **WRONG** - config specifies detection logic:

```javascript
{
  granularity: { variables: "all" },
  detectMisconceptions: true,  // ‚ùå This is assessment logic
  misconceptionTypes: ["hoisting", "coercion"]  // ‚ùå This is pedagogy
}
```

‚úÖ **CORRECT** - config specifies data capture:

```javascript
{
  granularity: {
    variables: "all",  // ‚úÖ What to capture
    expressions: true  // ‚úÖ Needed for coercion data
  }
}
```

**Anti-pattern 3: Grading rubric in documentation**

‚ùå **WRONG** - embody docs prescribe scoring:

```markdown
### Naming Quality Assessment

Award 0 points for single-letter names, 1 point for descriptive names.
```

‚úÖ **CORRECT** - embody docs describe data:

```markdown
### Naming Quality Assessment

`variable.declare` events provide `name` field. Tools implement naming quality rubrics based on pedagogical goals.
```

**Show 5-7 anti-patterns with corrections.**

**Deliverable**: Anti-patterns section

---

### Task 3.4: Integration with Phase 5 boundary docs (Researcher does)

**Section 5: Relationship to General Boundary Guidance** (~150 words)

Cross-reference Phase 5 boundary compliance docs:

```markdown
## Relationship to Phase 5 Boundary Documentation

This document extends the boundary principles established in `/5-learning-tools-and-environments/`:

- **trace-event-schemas.md**: Defines the low-level event types used in assessment contexts. Assessment use cases reference these same event types without modification.

- **trace-configuration.md**: Defines granularity and filter options. Assessment configs use these existing options; no assessment-specific config extensions are needed (identified in Phase 1 gap analysis).

- **PHASE5-BOUNDARY-COMPLIANCE.md**: Establishes general infrastructure/intelligence boundary. This document applies those principles specifically to assessment contexts (misconception detection, quality scoring, adaptive feedback).

**Consistency**: Assessment tools follow the same boundary as learning tools. The difference is in pedagogical PURPOSE (assessment vs practice), not in boundary placement.
```

**Deliverable**: Integration section

---

### Task 3.5: Practical guidance for tool builders (Developer + Educator)

**Section 6: Guidance for Assessment Tool Developers** (~300 words)

Actionable advice for maintaining boundary:

````markdown
## Practical Guidance for Assessment Tool Developers

### Step 1: Identify Your Assessment Goal (Pedagogical)

- What misconception are you detecting?
- What quality dimension are you scoring?
- What threshold state are you classifying?
- What feedback are you generating?

**This is YOUR responsibility** (tool-side intelligence).

### Step 2: Determine Required Execution Data (Technical)

- What execution behaviors reveal that misconception/quality/state?
- What events capture those behaviors?
- What event fields are needed for analysis?
- What config granularity balances data needs vs performance?

**Consult Column 4** in assessment-use-cases-table.md for common patterns.

### Step 3: Configure Embody (Infrastructure)

```javascript
const trace = embody(studentCode, {
  granularity: {
    /* your data needs */
  },
  filters: {
    /* focus on relevant code */
  }
});
```
````

**embody returns neutral execution events.**

### Step 4: Implement Assessment Algorithm (Intelligence)

```javascript
function assessQuality(trace) {
  const patterns = analyzeTracePatterns(trace);
  const score = applyRubric(patterns);
  const feedback = generateFeedback(score, patterns);
  return { score, feedback };
}
```

**This is YOUR responsibility** (tool-side intelligence).

### Decision Tree: Is This Embody's Job or Mine?

```
Does it involve pedagogical judgment?
‚îú‚îÄ YES ‚Üí Your job (tool-side)
‚îî‚îÄ NO ‚Üí Is it observable in execution?
   ‚îú‚îÄ YES ‚Üí Embody provides it (if event type exists)
   ‚îî‚îÄ NO ‚Üí Outside scope (e.g., student demographics, prior knowledge)
```

### Common Mistakes:

1. **Asking embody to detect misconceptions** ‚Üí No, embody provides data, YOU detect
2. **Asking embody to score quality** ‚Üí No, embody provides naming/API/structure data, YOU score
3. **Expecting embody to generate feedback** ‚Üí No, embody provides execution facts, YOU craft pedagogy

````

**Deliverable**: Practical guidance section

---

### Task 3.6: Quality checklist (All three create together)

**Section 7: Boundary Compliance Checklist** (~100 words)

Checklist for self-review:

```markdown
## Boundary Compliance Checklist

Use this checklist when writing assessment tool code or documentation:

**For event type proposals**:
- [ ] Event captures observable execution fact (not interpretation)
- [ ] Event data is value-neutral (no "good" or "bad" judgments)
- [ ] Event does not prescribe pedagogical action
- [ ] Event is needed for multiple use cases (not overly specific)

**For configuration options**:
- [ ] Option controls WHAT data is captured (not HOW it's interpreted)
- [ ] Option is value-neutral (no "detect misconceptions" flags)
- [ ] Option balances data needs with performance
- [ ] Option follows existing naming/structure patterns

**For documentation**:
- [ ] Describes WHAT embody provides (events, fields, structure)
- [ ] Does NOT prescribe HOW tools should use data
- [ ] Includes "Tools implement..." boundary marker
- [ ] Examples show embody data + separate tool logic

**For assessment algorithms**:
- [ ] Implemented in TOOL code (not embody)
- [ ] Consumes embody trace as input
- [ ] Does not modify trace data
- [ ] Clearly separated from data provision
````

**Deliverable**: Checklist section

---

### Phase 3 Summary

**Deliverable**: `assessment-boundary-compliance.md` (~1,500 words total)

**Sections**:

1. Introduction
2. Core boundary principle
3. Boundary-compliant examples (5-7 examples)
4. Anti-patterns to avoid (5-7 violations)
5. Integration with Phase 5 docs
6. Practical guidance for tool builders
7. Compliance checklist

**Quality Gates**:

- [ ] Examples clearly show embody vs tool responsibility
- [ ] Anti-patterns are recognizable mistakes developers might make
- [ ] Guidance is actionable (developers can follow it)
- [ ] Checklist is complete (covers all boundary aspects)
- [ ] Document references Phase 5 docs correctly
- [ ] Team consensus that this clarifies the boundary

---

## Phase 4: Integration, Polish, and Completion

### Goal

Integrate Step 5 work with existing Step 4 docs, create completion summary, prepare for future preset work.

### Task 4.1: Update Step 4 documents (Educator leads)

**File 1: `assessment-strategies-for-educators.md`**

Add new section after line 265 (after "Quick Start" section, before "Category Descriptions"):

```markdown
---

## Using Trace Data for Assessment (Step 5 Integration)

**What changed**: Assessment strategies now include **technical specifications** showing how execution trace data enables each assessment approach.

**For educators**: You now have visibility into:

- **What execution behaviors reveal misconceptions** (e.g., TDZ access indicates hoisting confusion)
- **What data enables quality assessment** (e.g., identifier names from variable declarations enable naming evaluation)
- **How tools detect threshold states** (e.g., closure capture patterns reveal closure understanding)

**Reading Column 4** ("How Trace Data Enables"):

- **Events**: Low-level execution events that capture relevant behaviors
- **Config**: Settings controlling which events are captured
- **Data**: Specific event fields that enable the assessment
- **Tools consume...**: Marks the boundary - after this, assessment algorithms are tool-specific

**Example - Misconception detection**:

- **embody provides**: Events showing TDZ access, type coercion, closure capture in loops
- **Assessment tool implements**: Pattern matching to identify specific misconceptions, feedback generation

**Why this matters**: Understanding the data foundation helps you:

1. Evaluate whether assessment tools have sufficient data for claims
2. Request specific trace configurations from tool developers
3. Understand limitations (if trace doesn't capture it, tool can't assess it)
4. Design assessment tasks that produce informative execution traces

**Next steps**: See `assessment-use-cases-table.md` Column 4 for technical details on each assessment approach.
```

**Deliverable**: Updated educator guide with Step 5 integration section

---

**File 2: `assessment-use-cases-companion-guide.md`**

Update "Guide to Reading Use Case Table" section (around line 150):

```markdown
## Guide to Reading the Use Case Table (4 Columns)

The assessment use cases table now has **4 columns** (Step 5 complete):

### Column 1: Use Case

Descriptive name of the assessment approach.

### Column 2: Evidence

- üî¨ **Established**: Direct research backing with empirical validation
- üìê **Framework**: Established theory applied to assessment context
- üß™ **Extension**: Logical but not yet validated

### Column 3: What Educational Tools Do

**Pedagogical function** - the educational goal of the assessment.

**Boundary**: This is TOOL responsibility (intelligence layer).

**Examples**:

- "Detect misconception patterns and provide targeted explanations"
- "Score code quality dimensions beyond correctness"
- "Classify threshold state and adapt feedback accordingly"

### Column 4: How Trace Data Enables (NEW in Step 5)

**Technical specification** - the execution data that makes the assessment possible.

**Boundary**: This is EMBODY responsibility (infrastructure layer).

**Format**:

- **Events**: Specific trace event types (e.g., `variable.declare`, `function.call`)
- **Config**: Granularity and filter settings
- **Data**: Key event fields enabling the assessment
- **Tools consume...**: Boundary marker - everything after this is tool-side

**Example**:
```

**Events**: `variable.declare`, `function.call`.
**Config**: `{ granularity: { variables: "declarations-only", functions: "user-code-only" } }`.
**Data**: Variable/function names enable quality assessment.
Tools consume events ‚Üí implement naming rubric ‚Üí generate quality scores.

```

**Reading tips**:
1. Column 3 tells you the WHAT (educational goal)
2. Column 4 tells you the HOW (data foundation)
3. The "Tools consume..." marker separates infrastructure from intelligence
4. If you're building assessment tools, Column 4 is your technical spec
5. If you're evaluating tools, Column 4 shows what data they have access to
```

**Deliverable**: Updated companion guide with 4-column explanation

---

**File 3: `categorization-synthesis-and-integration.md`**

Add trace data considerations to configuration profiles (around line 100, after each profile):

For each of the 6 profiles, add:

**Example for Profile 1: Threshold Liminal State Support**:

```markdown
**Trace data requirements** (from Step 5):

- **Events**: `closure.capture`, `scope.create`, `async.*`, `function.call` (for threshold-specific patterns)
- **Config**: `{ granularity: { functions: "all", scopes: "all", closures: "all", async: "await-only" } }`
- **Rationale**: Threshold state detection requires comprehensive capture of threshold-relevant events (closures, scopes, async) but can skip irrelevant detail (expressions, detailed control flow)
- **Performance**: Moderate overhead acceptable (classroom scale, diagnostic purpose, not continuous)
```

Add this subsection to all 6 profiles.

**Deliverable**: Updated synthesis doc with trace requirements per profile

---

### Task 4.2: Update README navigation (Developer does)

Update `/theory-to-requirements/6-assessment-strategies/README.md`:

Around line 30 (in "How to Navigate Step 4" section), update to "How to Navigate Steps 4-5":

```markdown
## How to Navigate Steps 4-5

Assessment strategies documentation provides **two complementary layers** with **pedagogical goals (Step 4)** now connected to **technical specifications (Step 5)**:

### Layer 1: Practical Quick Reference

**Start here if you need immediate, actionable guidance:**

1. **assessment-strategies-for-educators.md** (Updated) - Entry point guide
   - NOW INCLUDES: "Using Trace Data for Assessment" section explaining technical foundation

2. **assessment-use-cases-companion-guide.md** (Updated) - Scenario-based guide
   - NOW INCLUDES: How to read 4-column table (including Column 4 technical specs)

3. **assessment-use-cases-table.md** (Step 5 Complete) - Core deliverable
   - **50 use cases** with full 4-column specifications
   - Column 4 provides technical trace requirements for each use case

### Layer 2: Scholarly Analysis & Integration

**For comprehensive understanding and research grounding:**

4. **categorization-by-assessment-purpose.md** (700 lines) - Organize by WHY assess
5. **categorization-by-delivery-timing.md** (360 lines) - Organize by WHEN assess
6. **categorization-by-feedback-type.md** (400 lines) - Organize by HOW give feedback
7. **categorization-by-scale-deployment.md** (340 lines) - Organize by WHAT scale
8. **categorization-synthesis-and-integration.md** (Updated) - How to combine lenses
   - NOW INCLUDES: Trace data requirements for each configuration profile
9. **framework-comparison-and-integration.md** (500 lines) - How frameworks relate
10. **task-examples-and-scenarios.md** (400 lines) - Concrete assessment examples

### Layer 3: Technical Specifications (Step 5)

**For assessment tool developers and researchers:**

11. **assessment-boundary-compliance.md** (NEW) - Infrastructure/intelligence boundary guidance
    - Examples of correct boundary placement
    - Anti-patterns to avoid
    - Practical guidance for tool builders

12. **STEP5-COMPLETION-SUMMARY.md** (NEW) - Achievement overview
    - Event coverage analysis
    - Config patterns identified
    - Lines of documentation
    - Future work (presets, user guide, wrapper library)

### Navigation by Role

**Educators**: Start with Layer 1 (#1-3), reference Layer 2 for depth
**Tool Developers**: Focus on Layer 3 (#11), reference Layer 1 (#3) for use cases
**Researchers**: Read Layer 2 for theory, Layer 3 for technical grounding
**Students/Learners**: Layer 1 (#1-2) for understanding assessment approaches
```

**Deliverable**: Updated README with 3-layer navigation

---

### Task 4.3: Create Step 5 completion summary (Researcher leads, all contribute)

Create file: `STEP5-COMPLETION-SUMMARY.md`

**Section 1: Overview** (~100 words)

```markdown
# Step 5 Completion Summary: Technical Requirements Translation

**Date**: [Completion date]
**Status**: ‚úÖ Complete

**Achievement**: Added "How Trace Data Enables" column to 50 assessment use cases, completing the theory-to-requirements translation for assessment strategies.

**Core deliverable**: 4-column use case table connecting:

- Column 3 (Step 4): Educational goals (WHAT tools accomplish pedagogically)
- Column 4 (Step 5): Technical specifications (HOW trace data enables those goals)

**Boundary maintained**: Infrastructure (embody provides neutral events) vs Intelligence (tools implement assessment algorithms) clearly separated throughout.
```

**Section 2: Event Coverage Analysis** (~200 words)

```markdown
## Event Coverage Analysis

**Primary finding**: Existing Phase 5 trace events cover **~95% of assessment needs** with minimal gaps.

### Event Types by Category

| Event Category    | Use Cases Needing | Coverage Assessment                                               |
| ----------------- | ----------------- | ----------------------------------------------------------------- |
| Variable events   | 38/50 (76%)       | ‚úÖ Complete (`declare`, `assign`, `read`, `update`, `tdz-access`) |
| Function events   | 35/50 (70%)       | ‚úÖ Complete (`call`, `return`, `throw`)                           |
| Control flow      | 28/50 (56%)       | ‚úÖ Complete (`conditional.branch`, `loop.*`)                      |
| Scope events      | 22/50 (44%)       | ‚úÖ Complete (`create`, `enter`, `exit`)                           |
| Async events      | 18/50 (36%)       | ‚úÖ Complete (`await.*`, `promise.*`, `microtask.*`)               |
| Object events     | 15/50 (30%)       | ‚úÖ Complete (`create`, `property.access`, `prototype.lookup`)     |
| Error events      | 12/50 (24%)       | ‚úÖ Complete (`throw`, `catch`, `context`)                         |
| Expression events | 9/50 (18%)        | ‚úÖ Complete (`binary`, `unary`)                                   |

### Event Type Gaps Identified

[If any gaps found in Phase 1, list here. If none:]
**Zero new event types required**. Existing 30+ low-level events comprehensively cover assessment use cases.

### Config Option Gaps Identified

[If any gaps found in Phase 1, list here. If none:]
**Minimal config additions** (if any). Existing granularity/filter/serialization options sufficient for assessment needs.
```

**Section 3: Config Patterns Identified** (~300 words)

````markdown
## Configuration Patterns Identified

Analysis of 50 use case configs revealed **7 recurring patterns** that will inform future preset design (separate step):

### Pattern 1: Comprehensive Diagnostic (~12 use cases)

**Use cases**: Misconception detection, threshold state diagnosis, NM understanding
**Config profile**:

```javascript
{
  granularity: {
    variables: "all",
    functions: "all",
    scopes: "all",
    controlFlow: "detailed",
    async: "all",
    objects: "all",
    errors: "all",
    expressions: true  // High overhead but needed for coercion detection
  },
  filters: { excludeBuiltins: true }
}
```
````

**Trade-off**: High overhead, comprehensive data
**Context**: Diagnostic assessment, small scale acceptable

### Pattern 2: Quality-Focused Minimal (~6 use cases)

**Use cases**: QLCs (naming, API, algorithms, structure)
**Config profile**:

```javascript
{
  granularity: {
    variables: "declarations-only",  // Just names
    functions: "user-code-only",     // API choices
    controlFlow: "detailed",         // Algorithms
    scopes: "none",                  // Irrelevant
    async: "none",
    objects: "creation-only",
    errors: "throws-only",
    expressions: false
  },
  filters: { excludeBuiltins: true },
  serialization: { values: "shallow", maxDepth: 1 }
}
```

**Trade-off**: Minimal overhead, focused data
**Context**: Automated grading at scale

[Continue for all 7 patterns...]

### Pattern Summary

These patterns are **documentation only** at this stage. Future work (separate requirements step) will translate these into educator/developer-friendly presets that map to granular configs while preserving embody's low-level boundary.

````

**Section 4: Documentation Produced** (~100 words)

```markdown
## Documentation Produced

### Core Deliverables
1. **assessment-use-cases-table.md** - Updated from 450 to ~750 lines
   - Added Column 4 to all 50 use cases
   - Updated header sections with guidance
   - Maintained boundary compliance throughout

2. **assessment-boundary-compliance.md** - NEW, ~1,500 words
   - Core boundary principles for assessment
   - 5-7 compliant examples
   - 5-7 anti-patterns
   - Practical guidance for tool builders
   - Compliance checklist

3. **STEP5-COMPLETION-SUMMARY.md** - This document

### Updated Documents (Step 4 Integration)
4. **assessment-strategies-for-educators.md** - Added trace data section
5. **assessment-use-cases-companion-guide.md** - Updated table reading guide
6. **categorization-synthesis-and-integration.md** - Added trace requirements to profiles
7. **README.md** - Updated navigation for Steps 4-5

### Temporary Working Documents (Not final deliverables)
- `assessment-event-mapping-work.md` - Phase 1 analysis (can archive)
- `event-coverage-matrix.md` - Coverage visualization (can archive)
- `config-patterns-analysis.md` - Pattern analysis (informs future preset work)

**Total new documentation**: ~2,000 lines
**Total updated documentation**: ~1,000 lines revised
````

**Section 5: Quality Assurance** (~200 words)

```markdown
## Quality Assurance

### Boundary Compliance Audit

**Result**: ‚úÖ Zero violations across all 50 use cases

**Checks performed**:

- [ ] ‚úÖ All Column 4 entries specify low-level events (not "detect bugs")
- [ ] ‚úÖ All configs use existing granularity options (verified against Phase 5)
- [ ] ‚úÖ All event types referenced exist in Phase 5 schemas
- [ ] ‚úÖ All entries include "Tools consume..." boundary marker
- [ ] ‚úÖ No assessment algorithms specified in embody territory
- [ ] ‚úÖ No feedback content prescribed
- [ ] ‚úÖ No UI/visualization requirements
- [ ] ‚úÖ No grading rubrics or thresholds

### Completeness Check

- [ ] ‚úÖ All 50 use cases have Column 4 entries
- [ ] ‚úÖ All categories (1-10) covered
- [ ] ‚úÖ All frameworks (misconceptions, NMs, thresholds, taxonomies, QLCs) represented
- [ ] ‚úÖ Event coverage analysis complete
- [ ] ‚úÖ Config patterns identified

### Consistency Check

- [ ] ‚úÖ Column 4 format consistent across all entries
- [ ] ‚úÖ Event naming matches Phase 5 schemas exactly
- [ ] ‚úÖ Config syntax is valid JavaScript object notation
- [ ] ‚úÖ Terminology consistent with Phase 5 docs

### Research Grounding Check

- [ ] ‚úÖ Evidence classifications maintained from Step 4 (üî¨/üìê/üß™)
- [ ] ‚úÖ Technical specs support pedagogical claims from Column 3
- [ ] ‚úÖ No embody-side claims without event evidence
```

**Section 6: Future Work** (~200 words)

```markdown
## Future Work (Separate from This Step)

### 1. Preset Design (Separate Requirements Step)

**Purpose**: Map educator/developer needs to granular configs
**Approach**: Separate directory, preserves low-level boundary
**Scope**: All subdirectories (learning tools + assessment + others)
**Input**: Config patterns identified in this step
**Output**: User-friendly presets with pedagogical descriptions that compile to granular configs

**Examples**:

- "Misconception detection preset" ‚Üí comprehensive diagnostic config
- "MOOC grading preset" ‚Üí minimal quality-focused config with sampling
- "Threshold support preset" ‚Üí threshold-event-focused config

**Critical**: Presets are a **convenience layer**, not a boundary violation. They map to existing granular configs.

### 2. User Guide Development

**Purpose**: Explain pedagogical use cases with trace data examples
**Audience**: Educators, tool builders, researchers
**Content**:

- How misconceptions appear in traces
- How threshold concepts manifest in execution patterns
- Which configs to use for which assessment goals
- Learning environment design hints

### 3. Wrapper Library (Backlog)

**Purpose**: Usecase-based wrapper around Embody + Examine
**Scope**: Handles config for both libraries + initial pedagogical analysis
**Boundary**: Extends into userland without violating embody's neutral infrastructure boundary
**Status**: Backlog (future work)

### 4. Preset Validation

**Purpose**: Implement and test the presets identified in config pattern analysis
**Scope**: Performance benchmarking, use case coverage verification
**Status**: After preset requirements step complete
```

**Deliverable**: Complete `STEP5-COMPLETION-SUMMARY.md`

---

### Task 4.4: Archive working documents (Developer does)

1. Create directory: `/theory-to-requirements/6-assessment-strategies/step5-working-docs/`
2. Move temporary documents:
   - `assessment-event-mapping-work.md`
   - `event-coverage-matrix.md`
   - `config-patterns-analysis.md`
   - `column-4-writing-guide.md`
   - `event-type-reference.md`
   - `config-options-reference.md`
   - `performance-considerations.md`
3. Add README in working-docs:

   ```markdown
   # Step 5 Working Documents

   These documents supported Step 5 development but are not final deliverables.

   - **assessment-event-mapping-work.md**: Detailed mapping of 50 use cases to events (Phase 1 output)
   - **event-coverage-matrix.md**: Visualization of event type usage across use cases
   - **config-patterns-analysis.md**: Identified configuration archetypes (informs future preset work)
   - **column-4-writing-guide.md**: Writing standards for team calibration
   - **event-type-reference.md, config-options-reference.md**: Phase 5 infrastructure study notes
   - **performance-considerations.md**: Overhead analysis by config pattern

   **Status**: Archived for reference, not maintained
   ```

**Deliverable**: Organized working documents

---

### Phase 4 Summary

**Deliverables**:

1. Updated `assessment-strategies-for-educators.md` with trace data section
2. Updated `assessment-use-cases-companion-guide.md` with 4-column guide
3. Updated `categorization-synthesis-and-integration.md` with trace requirements
4. Updated `README.md` with 3-layer navigation
5. `STEP5-COMPLETION-SUMMARY.md` - comprehensive achievement summary
6. Archived working documents in `step5-working-docs/`

**Quality Gates**:

- [ ] All Step 4 docs integrated with Step 5 content
- [ ] Navigation clear for all user roles
- [ ] Completion summary is comprehensive
- [ ] Working docs archived (not cluttering main directory)
- [ ] Team consensus on completeness

---

## Phase 5: Self-Review and Quality Assurance

### Goal

Systematic quality review before declaring Step 5 complete, ensuring boundary compliance, completeness, consistency, and implementability.

### Task 5.1: Boundary compliance audit (Researcher leads)

Create file: `STEP5-QUALITY-REVIEW.md`

**Section 1: Boundary Compliance Audit**

Check ALL 50 use cases against violation patterns:

```markdown
## Boundary Compliance Audit

**Audit date**: [Date]
**Auditor**: [Researcher name]

### Violation Pattern Checks (50 use cases reviewed)

#### Pattern 1: Pedagogical interpretation in event specifications

**Check**: Do any Column 4 entries include quality judgments, misconception labels, or feedback prescriptions in event data?

**Method**: Search for keywords: "good", "bad", "correct", "incorrect", "misconception", "feedback", "hint", "explain"

**Result**: [X violations found OR ‚úÖ Zero violations]
[If violations found, list use case + correction needed]

#### Pattern 2: Assessment algorithms in embody territory

**Check**: Do any entries prescribe HOW to detect patterns rather than WHAT data enables detection?

**Method**: Look for algorithm descriptions before "Tools consume..." marker

**Result**: [X violations found OR ‚úÖ Zero violations]

#### Pattern 3: UI/visualization requirements

**Check**: Do any entries mandate specific visual representations or interface designs?

**Method**: Search for: "display", "show", "visualize", "render", "dashboard", "interface"

**Result**: [X violations found OR ‚úÖ Zero violations]

#### Pattern 4: Grading rubrics or thresholds

**Check**: Do any entries specify scoring rules, cutoff points, or grading criteria?

**Method**: Search for: "score", "points", "threshold", "cutoff", "passing", "rubric"

**Result**: [X violations found OR ‚úÖ Zero violations]

#### Pattern 5: Feedback content specification

**Check**: Do any entries prescribe exact feedback text or hint progression strategies?

**Method**: Look for quoted feedback text or multi-step hint sequences

**Result**: [X violations found OR ‚úÖ Zero violations]

#### Pattern 6: Tool-specific requirements

**Check**: Do any entries require specific assessment tool implementations or vendor-specific features?

**Method**: Look for tool names, proprietary algorithms, specific product mentions

**Result**: [X violations found OR ‚úÖ Zero violations]

### Overall Boundary Compliance Score

**Violations found**: [Total across all patterns]
**Total boundary markers checked**: 50 (one per use case)
**Compliance rate**: [X%]

**Target**: 100% compliance (zero violations)
**Status**: [‚úÖ Pass OR ‚ùå Revisions needed]
```

**If violations found**: Document each, assign to team member for correction, re-audit after fixes.

**Deliverable**: Completed boundary audit section

---

### Task 5.2: Completeness verification (All three review together)

**Section 2: Completeness Verification**

```markdown
## Completeness Verification

### Use Case Coverage

- [ ] ‚úÖ All 50 use cases have Column 4 entries
- [ ] ‚úÖ All 10 categories covered (Formative, Summative, Diagnostic, QLCs, Process, Comprehension, Authentic, Adaptive, Fairness, Multi-dimensional)
- [ ] ‚úÖ All 5 frameworks represented (Misconceptions, NMs, Thresholds, Taxonomies, QLCs)

### Event Type Coverage

- [ ] ‚úÖ Variable events: Used by [X] use cases
- [ ] ‚úÖ Function events: Used by [X] use cases
- [ ] ‚úÖ Scope events: Used by [X] use cases
- [ ] ‚úÖ Control flow events: Used by [X] use cases
- [ ] ‚úÖ Async events: Used by [X] use cases
- [ ] ‚úÖ Object events: Used by [X] use cases
- [ ] ‚úÖ Error events: Used by [X] use cases
- [ ] ‚úÖ Expression events: Used by [X] use cases

**Finding**: [All event categories utilized OR some categories unused - explain why]

### Config Option Coverage

- [ ] ‚úÖ Granularity dimensions: All 8 used appropriately
- [ ] ‚úÖ Filters: Used where appropriate (builtin exclusion, source filtering)
- [ ] ‚úÖ Serialization: Depth/detail varied by use case needs
- [ ] ‚úÖ Performance: Sampling/limits used for MOOC scale use cases

### Documentation Coverage

- [ ] ‚úÖ Core table updated (assessment-use-cases-table.md)
- [ ] ‚úÖ Boundary guidance created (assessment-boundary-compliance.md)
- [ ] ‚úÖ Educator guide updated (assessment-strategies-for-educators.md)
- [ ] ‚úÖ Companion guide updated (assessment-use-cases-companion-guide.md)
- [ ] ‚úÖ Synthesis doc updated (categorization-synthesis-and-integration.md)
- [ ] ‚úÖ README navigation updated
- [ ] ‚úÖ Completion summary created (STEP5-COMPLETION-SUMMARY.md)

### Gap Documentation

- [ ] ‚úÖ Event type gaps documented (if any)
- [ ] ‚úÖ Config option gaps documented (if any)
- [ ] ‚úÖ Out-of-scope use cases noted (Process Assessment category - development environment concerns)

**Status**: [‚úÖ Complete OR ‚ùå Gaps remain]
```

**Deliverable**: Completeness verification section

---

### Task 5.3: Consistency check (Developer leads)

**Section 3: Consistency Check**

```markdown
## Consistency Verification

### Format Consistency (Column 4 entries)

**Method**: Sample 10 random use cases, check format adherence

**Format template**:
```

**Events**: [types]. **Config**: [object]. **Data**: [fields]. Tools consume ‚Üí implement [algorithm] ‚Üí generate [output].

```

**Sample checks**:
1. Use case [X]: [‚úÖ Pass OR ‚ùå Fail - reason]
2. Use case [Y]: [‚úÖ Pass OR ‚ùå Fail - reason]
...
10. Use case [Z]: [‚úÖ Pass OR ‚ùå Fail - reason]

**Consistency rate**: [X/10]
**Status**: [‚úÖ Pass (‚â•9/10) OR ‚ùå Needs revision]

### Terminology Consistency
**Check**: Event type names match Phase 5 exactly

**Method**: Extract all event type references from Column 4, cross-check with `/5-learning-tools-and-environments/trace-event-schemas.md`

**Mismatches found**: [List any OR ‚úÖ Zero mismatches]

### Config Syntax Consistency
**Check**: All config objects use valid JavaScript syntax

**Method**: Attempt to parse each config as JSON

**Syntax errors found**: [List any OR ‚úÖ Zero errors]

### Cross-Reference Consistency
**Check**: References to Phase 5 docs are correct

**Method**: Verify all document references point to existing files/sections

**Broken references found**: [List any OR ‚úÖ Zero broken references]

**Status**: [‚úÖ Pass OR ‚ùå Needs fixes]
```

**Deliverable**: Consistency check section

---

### Task 5.4: Research grounding verification (Researcher leads)

**Section 4: Research Grounding Verification**

```markdown
## Research Grounding Verification

### Evidence Classifications Maintained

**Check**: Column 2 evidence markers (üî¨/üìê/üß™) from Step 4 still present

**Result**: [‚úÖ All maintained OR ‚ùå Some lost in updates]

### Technical Specs Support Pedagogical Claims

**Method**: For each use case, verify Column 4 (technical) actually enables Column 3 (pedagogical)

**Sample analysis** (5 use cases across categories):

1. **Misconception-triggered feedback** (Cat 1):
   - Column 3 claim: "Detect misconceptions, provide targeted feedback"
   - Column 4 support: TDZ events, coercion flags, closure capture ‚Üí ‚úÖ Sufficient for detection
   - Assessment: [‚úÖ Supported OR ‚ùå Insufficient data]

2. **Quality-based grading** (Cat 2):
   - Column 3 claim: "Assess naming, API, algorithms, patterns"
   - Column 4 support: Variable declarations (names), function calls (API), control flow (algorithms) ‚Üí ‚úÖ Sufficient
   - Assessment: [‚úÖ Supported OR ‚ùå Insufficient data]

[Repeat for 3 more use cases...]

**Overall assessment**: [‚úÖ Technical specs consistently support pedagogical claims OR ‚ùå Gaps found]

### Alignment with Published Research

**Check**: Use cases reference correct research (Step 4 citations still relevant with Step 5 additions)

**Key frameworks checked**:

- [ ] ‚úÖ QLCs (Lehtinen et al. 2023) - Quality dimension events correctly specified
- [ ] ‚úÖ Misconceptions (Qian & Lehman 2017) - Detection data available
- [ ] ‚úÖ Threshold concepts (Meyer & Land 2003) - State indicators capturable
- [ ] ‚úÖ Notional machines (Sorva 2013) - NM-specific events identified

**Status**: [‚úÖ Pass OR ‚ùå Misalignments found]
```

**Deliverable**: Research grounding section

---

### Task 5.5: Implementability assessment (Developer + Educator)

**Section 5: Implementability Assessment**

```markdown
## Implementability Assessment

### Developer Perspective (Can this be implemented?)

**Question 1**: Can a developer configure embody based on Column 4 specs?

**Test method**: Developer attempts to write embody config for 3 use cases without additional documentation

**Results**:

- Use case 1 [name]: [‚úÖ Successfully configured OR ‚ùå Needed clarification]
- Use case 2 [name]: [‚úÖ Successfully configured OR ‚ùå Needed clarification]
- Use case 3 [name]: [‚úÖ Successfully configured OR ‚ùå Needed clarification]

**Question 2**: Are event types clearly defined with accessible documentation?

**Check**: All event types in Column 4 are documented in Phase 5 with examples

**Result**: [‚úÖ All documented OR ‚ùå Missing docs for: [list]]

**Question 3**: Are config options within reasonable complexity?

**Check**: No configs require undocumented combinations or expert-level tuning

**Result**: [‚úÖ All reasonable OR ‚ùå Complex configs: [list]]

### Educator Perspective (Does this make pedagogical sense?)

**Question 1**: Can educators understand the data‚Üípedagogy connection?

**Test method**: Educator reads Column 4 for 3 use cases, explains in own words how trace enables assessment

**Results**:

- Use case 1 [name]: [‚úÖ Connection clear OR ‚ùå Unclear how data helps]
- Use case 2 [name]: [‚úÖ Connection clear OR ‚ùå Unclear how data helps]
- Use case 3 [name]: [‚úÖ Connection clear OR ‚ùå Unclear how data helps]

**Question 2**: Are assessment goals achievable with specified data?

**Check**: No use cases promise assessment capabilities that trace data can't support

**Result**: [‚úÖ All achievable OR ‚ùå Overclaims: [list]]

### Performance Reality Check

**Question**: Are high-overhead configs justified and noted?

**Check**: Any configs with `expressions: true` or comprehensive events include rationale

**Result**: [‚úÖ All justified OR ‚ùå Missing justification: [list]]

**Overall implementability**: [‚úÖ Ready for tool development OR ‚ùå Needs clarification]
```

**Deliverable**: Implementability assessment section

---

### Task 5.6: Final summary and recommendations (All three)

**Section 6: Quality Review Summary**

```markdown
## Quality Review Summary

**Review date**: [Date]
**Reviewers**: [Educator, Developer, Researcher names]

### Overall Assessment

| Quality Dimension   | Score             | Status  | Notes      |
| ------------------- | ----------------- | ------- | ---------- |
| Boundary Compliance | [X/50 violations] | [‚úÖ/‚ùå] | [Comments] |
| Completeness        | [X/X checks pass] | [‚úÖ/‚ùå] | [Comments] |
| Consistency         | [X%]              | [‚úÖ/‚ùå] | [Comments] |
| Research Grounding  | [‚úÖ/‚ùå]           | [‚úÖ/‚ùå] | [Comments] |
| Implementability    | [‚úÖ/‚ùå]           | [‚úÖ/‚ùå] | [Comments] |

**Overall score**: [X/100]

**Scoring**:

- 95-100: Production-ready, approved for implementation
- 85-94: Minor revisions needed
- 70-84: Moderate revisions needed
- <70: Major revisions needed

### Strengths Identified

1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

### Issues Requiring Correction

[If any issues found in review:]

1. [Issue 1] - Assigned to: [Name] - Due: [Date]
2. [Issue 2] - Assigned to: [Name] - Due: [Date]

[If no issues:]
‚úÖ Zero issues requiring correction

### Recommendations for Future Work

1. [Recommendation 1 - e.g., specific preset to prioritize]
2. [Recommendation 2 - e.g., user guide topic]
3. [Recommendation 3 - e.g., wrapper library feature]

### Sign-Off

**Educator**: [Name, Date, ‚úÖ Approved OR ‚ùå Revisions needed]
**Developer**: [Name, Date, ‚úÖ Approved OR ‚ùå Revisions needed]
**Researcher**: [Name, Date, ‚úÖ Approved OR ‚ùå Revisions needed]

**Step 5 Status**: [‚úÖ COMPLETE OR ‚ùå IN REVISION]
```

**Deliverable**: Complete `STEP5-QUALITY-REVIEW.md`

---

### Phase 5 Summary

**Deliverable**: `STEP5-QUALITY-REVIEW.md` (~2,000 words) with:

1. Boundary compliance audit
2. Completeness verification
3. Consistency check
4. Research grounding verification
5. Implementability assessment
6. Final summary and sign-off

**Quality Gates**:

- [ ] All 50 use cases audited for boundary compliance
- [ ] Completeness verified across all dimensions
- [ ] Consistency checked and issues corrected
- [ ] Research grounding confirmed
- [ ] Implementability validated by developer + educator
- [ ] Team sign-off achieved (all three approve)

**Final decision**: If all quality gates pass ‚Üí Step 5 COMPLETE. If issues found ‚Üí assign corrections, re-review, iterate until approval.

---

## Plan Summary

### Success Criteria (from original plan)

- ‚úÖ All 50 use cases have "How Trace Data Enables" entries
- ‚úÖ Zero boundary violations (no assessment algorithms in specs)
- ‚úÖ 100% event coverage (no use cases lack trace support)
- ‚úÖ Consistent format with Phase 5 learning tools
- ‚úÖ Research grounding maintained throughout
- ‚úÖ Implementable by tool developers (clear, concrete, actionable)

### Deliverables Summary

**Core deliverables** (must produce):

1. `assessment-use-cases-table.md` - Updated with Column 4 (~750 lines)
2. `assessment-boundary-compliance.md` - NEW (~1,500 words)
3. `STEP5-COMPLETION-SUMMARY.md` - NEW (~1,200 words)
4. `STEP5-QUALITY-REVIEW.md` - NEW (~2,000 words)

**Updated documents** (must revise): 5. `assessment-strategies-for-educators.md` - Add trace section 6. `assessment-use-cases-companion-guide.md` - Update table guide 7. `categorization-synthesis-and-integration.md` - Add trace requirements 8. `README.md` - Update navigation

**Working documents** (temporary, archive after use):
9-15. Various analysis and mapping documents (Phase 1-2 outputs)

**Total new documentation**: ~5,000 lines
**Total revised documentation**: ~1,500 lines

### Estimated Effort by Phase

- **Phase 1** (Event mapping): ~8-12 hours (most time-intensive, but well-prepared by research)
- **Phase 2** (Column 4 writing): ~6-8 hours (systematic, template-based)
- **Phase 3** (Boundary docs): ~4-6 hours (examples + guidance)
- **Phase 4** (Integration): ~3-4 hours (updates + summary)
- **Phase 5** (Quality review): ~4-6 hours (systematic audit)

**Total**: ~25-36 hours of collaborative work

### Team Collaboration Points

**Daily check-ins recommended** at:

1. End of Phase 1 (verify mapping quality before writing)
2. Middle of Phase 2 (calibrate writing after first 10-15 use cases)
3. End of Phase 3 (review boundary docs before integration)
4. Start of Phase 5 (coordinate audit responsibilities)

### Risk Mitigation

**Risk 1**: Boundary violations creep in during writing
**Mitigation**: Phase 2A calibration, Phase 5 systematic audit

**Risk 2**: Event type gaps discovered late
**Mitigation**: Phase 1C gap analysis before committing to writing

**Risk 3**: Inconsistent format across 50 use cases
**Mitigation**: Template in Phase 2A, consistency check in Phase 5

**Risk 4**: Insufficient detail for implementability
**Mitigation**: Phase 5 implementability test with actual config writing

**Risk 5**: Research grounding lost in technical translation
**Mitigation**: Phase 5 research grounding verification

### What Success Looks Like

**For Educators**:

- Can read Column 4 and understand what execution data reveals pedagogically
- Can evaluate whether assessment tools have sufficient data for their claims
- Can design assessment tasks that produce informative traces

**For Developers**:

- Can configure embody based solely on Column 4 specs
- Understand the boundary (what embody provides vs what they implement)
- Have concrete examples of boundary-compliant assessment tool architecture

**For Researchers**:

- Have complete mapping from assessment theory to technical requirements
- Can validate assessment tool claims against trace data availability
- Have foundation for future work (presets, user guide, wrapper library)

**For Embody**:

- Maintains pure infrastructure boundary (no pedagogy in core)
- Comprehensive assessment use case coverage (~95% with existing events)
- Clear path to ecosystem growth (presets, guides, wrappers separate from core)
